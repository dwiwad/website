[{"authors":["admin"],"categories":null,"content":"About Me I am a Postdoctoral Fellow in the Dispute Resolution Research Center of the Management and Organizations Department at the Kellogg School of Management, Northwestern University Primarily, my research focuses on uncovering the various psychological biases and misperceptions that legitimize economic inequality, even as it has risen to levels that negatively impact both organizations and society broadly. I subsequently leverage these insights to create interventions that foster egalitarian behaviors, emotions, and cognitions across public and private organizations. I am also an avid semi-professional cyclist; I compete at the highest level across both Canada and the United States on both the road and the velodrome. ","date":1549324800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1567641600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/dylan-wiwad/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/dylan-wiwad/","section":"authors","summary":"About Me I am a Postdoctoral Fellow in the Dispute Resolution Research Center of the Management and Organizations Department at the Kellogg School of Management, Northwestern University Primarily, my research focuses on uncovering the various psychological biases and misperceptions that legitimize economic inequality, even as it has risen to levels that negatively impact both organizations and society broadly.","tags":null,"title":"Dylan Wiwad","type":"authors"},{"authors":["吳恩達"],"categories":null,"content":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":1461110400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1555459200,"objectID":"da99cb196019cc5857b9b3e950397ca9","permalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","section":"authors","summary":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"吳恩達","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":"My Collaborators   Nour Kteily\nKellogg School of Management Shai Davidai\nColumbia Business School Nicole Stephens\nKellogg School of Management Jon Jachimowicz\nHarvard Business School Lara Aknin\nSimon Fraser University Azim Shariff\nUniversity of British Columbia Paul Piff\nUniversity of California, Irvine Brett Mercier\nUniversity of California, Irvine  ","date":1588654800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588654800,"objectID":"0e5a1f2452d505c1f0f986216d162459","permalink":"/collaborators/","publishdate":"2020-05-05T00:00:00-05:00","relpermalink":"/collaborators/","section":"","summary":"My Collaborators   Nour Kteily\nKellogg School of Management Shai Davidai\nColumbia Business School Nicole Stephens\nKellogg School of Management Jon Jachimowicz\nHarvard Business School Lara Aknin\nSimon Fraser University Azim Shariff","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"My Research  My research identifies the key role that organizations play in understanding economic inequality: they not only create and maintain economic inequality but are also uniquely susceptible to many of the negative consequences that arise when economic inequality approaches unproductive levels (e.g., higher health care costs, higher employee turnover, unhappier and less productive workers). In particular, my research has (i) uncovered various psychological mechanisms (e.g., income mobility, attributions for poverty) that legitimize inequality, (ii) tested how, when, and what interventions (e.g., perspective taking, cross-class contact) can promote egalitarian behavior and mitigate the negative consequences of economic inequality. You can see my full research statement here.\nPeer Reviewed Publications  2020  Wiwad, D., Mercier, B., Piff. P. K., Aknin, L. B., Mercier, B., \u0026amp; Shariff, A. F. (2020). Recognizing the impact of COVID-19 on the poor alters attitudes towards poverty and inequality. Journal of Experimental Social Psychology, 93, 104083. doi.org/10.1016/j.jesp.2020.104083\nData, Materials, and Code\nPiff. P. K*., \u0026amp; Wiwad, D.*, Robinson, A. R., Aknin, L. B., Mercier, B., \u0026amp; Shariff, A. F. (2020). Shifting attributions for poverty motivates opposition to inequality and enhances egalitarianism. Nature Human Behavior, 4, 496-505. doi.org/10.1038/s41562-020-0835-8\nData, Materials, and Code\nMercier, B., Wiwad, D., Aknin, L. B., Piff, P. K., \u0026amp; Shariff, A. F. (2020). Does belief in free will increase support for economic inequality? Collabra: Psychology, 6, 25. doi.org/10.1525/Collabra.303\nData, Materials, and Code\n 2019  Wiwad, D., Mercier, B., Maraun, M. D., Robinson, A. R., Piff, P. K., Aknin, L. B., \u0026amp; Shariff, A. F. (2019). The support for economic inequality scale: Development and adjudication. PLoS ONE, 14, e0218685. doi.org/10.1371/journal.pone.0218685\nData, Materials, and Code\nAknin, L. B., Wiwad, D., \u0026amp; Girme, Y. U. (2019). Not all gifts are good: The potential practical costs of motivated gifts. Journal of Applied Social Psychology, 49, 75-85. doi.org/10.1111/jasp.12566\nData, Materials, and Code\n 2018  Aknin, L. B., Wiwad, D., \u0026amp; Hanniball, K. (2018). Buying well-being: Spending behavior and happiness. Social and Personality Psychology Compass, 12, e12386. doi.org/10.1111/spc3.12386\n 2017  Wiwad, D., Aknin, L. B. (2017). Motives matter: The emotional consequences of recalled self- and other focused prosocial behavior. Motivation and Emotion, 41, 730-740. doi.org/10.1007/s11031-017-9638-2\nData, Materials, and Code\n 2016  Shariff, A. F., Wiwad, D., \u0026amp; Aknin, L. B. (2016). Income mobility breeds tolerance for economic inequality: Cross-national and experimental evidence. Perspectives on Psychological Science, 11, 373-380. doi.org/10.1177/1745691616635596\n","date":1588654800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588654800,"objectID":"fe675c30935c5c13de98eae9d03fbfad","permalink":"/publications_citestyle/","publishdate":"2020-05-05T00:00:00-05:00","relpermalink":"/publications_citestyle/","section":"","summary":"My Research  My research identifies the key role that organizations play in understanding economic inequality: they not only create and maintain economic inequality but are also uniquely susceptible to many of the negative consequences that arise when economic inequality approaches unproductive levels (e.","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"Teaching Philosophy  My goal when teaching is not only to foster a meaningful understanding of course content, but to ensure that students leave the classroom with knowledge they can confidently and successfully apply to their own lives. Over five years of teaching undergraduates at Simon Fraser University, and the past year of teaching MBA students at Kellogg School of Management, I have developed the two core philosophies of Learning by Doing and Inclusive Teaching in service of this goal. You can see my full teaching philosophy here.\n Courses Taught  Kellogg School of Management Negotiation Fundamentals, Part-Time MBA, Summer 2020 Simon Fraser University Introduction to Social Psychology, Winter 2019  TA Experience  Kellogg School of Management Negotiations Fundamentals, Full Time MBA (2x), Spring 2020 Simon Fraser University Introduction to Social Psychology, Teaching Assistant (x6), 2013 - 2019\nIntroduction to Data analysis in Psychology, Teaching Assistant (x2), 2014 - 2020\nIntroduction to Research Methods, Teaching Assistant (x7), 2016 - 2020\nPsychology of Intergroup Relations, Teaching Assistant, Winter 2015 - Fall 2016\n ","date":1588654800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588654800,"objectID":"322dbaccf72a6d71f827fdb2866be935","permalink":"/teaching/","publishdate":"2020-05-05T00:00:00-05:00","relpermalink":"/teaching/","section":"","summary":"Teaching Philosophy  My goal when teaching is not only to foster a meaningful understanding of course content, but to ensure that students leave the classroom with knowledge they can confidently and successfully apply to their own lives.","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"","date":1587513600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587513600,"objectID":"8493b949930b15f4abca95a27f3bac41","permalink":"/project/latimes_covid/","publishdate":"2020-04-22T00:00:00Z","relpermalink":"/project/latimes_covid/","section":"project","summary":"Our write up in The LA Times Daily Pilot about our the application of our new research to the Coronavirus Pandemic.","tags":["Popular Press"],"title":"Coronavirus crisis may help Americans remember that economic inequality is not fair or just","type":"project"},{"authors":null,"categories":null,"content":"","date":1584316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584316800,"objectID":"7259bdbdbb641d87cf5fb49ee9401845","permalink":"/project/nhb_behindthepaper/","publishdate":"2020-03-16T00:00:00Z","relpermalink":"/project/nhb_behindthepaper/","section":"project","summary":"A behind the scenes look at our new paper in Nature Human Behavior.","tags":["Popular Press"],"title":"Why are the Poor Poor and Why Does it Matter?","type":"project"},{"authors":["Paul K. Piff*","Dylan Wiwad*","Angela R. Robinson","Lara B. Aknin","Brett Mercier","Azim Shariff"],"categories":null,"content":"","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"b33897c8f4a0aaf5b88fc8dfe5219cdf","permalink":"/publication/nhb_2020/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/nhb_2020/","section":"publication","summary":"*Nature Human Behavior*, March 2020","tags":null,"title":"Shifting attributions for poverty motivates opposition to inequality and enhances egalitarianism","type":"publication"},{"authors":null,"categories":null,"content":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you\u0026rsquo;ll find some examples of the types of technical content that can be rendered with Academic.\nExamples Code Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the highlight option in your config/_default/params.toml file.\n```python import pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head() ```  renders as\nimport pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head()  Math Academic supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.toml file.\nTo render inline or block math, wrap your LaTeX math with $...$ or $$...$$, respectively.\nExample math block:\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |} {\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$  renders as\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left |\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right |^2}$$\nExample inline math $\\nabla F(\\mathbf{x}_{n})$ renders as $\\nabla F(\\mathbf{x}_{n})$.\nExample multi-line math using the \\\\\\\\ math linebreak:\n$$f(k;p_0^*) = \\begin{cases} p_0^* \u0026amp; \\text{if }k=1, \\\\\\\\ 1-p_0^* \u0026amp; \\text {if }k=0.\\end{cases}$$  renders as\n$$f(k;p_0^*) = \\begin{cases} p_0^* \u0026amp; \\text{if }k=1, \\\\\n1-p_0^* \u0026amp; \\text {if }k=0.\\end{cases}$$\nDiagrams Academic supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid graph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] ```  renders as\ngraph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2]  An example sequence diagram:\n```mermaid sequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! ```  renders as\nsequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good!  An example Gantt diagram:\n```mermaid gantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d ```  renders as\ngantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d  An example class diagram:\n```mermaid classDiagram Class01 \u0026lt;|-- AveryLongClass : Cool \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Class01 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; int id size() } ```  renders as\nclassDiagram Class01 \u0026lt;|-- AveryLongClass : Cool \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Class01 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; int id size() }  An example state diagram:\n```mermaid stateDiagram [*] --\u0026gt; Still Still --\u0026gt; [*] Still --\u0026gt; Moving Moving --\u0026gt; Still Moving --\u0026gt; Crash Crash --\u0026gt; [*] ```  renders as\nstateDiagram [*] --\u0026gt; Still Still --\u0026gt; [*] Still --\u0026gt; Moving Moving --\u0026gt; Still Moving --\u0026gt; Crash Crash --\u0026gt; [*]  Todo lists You can even write your todo lists in Academic too:\n- [x] Write math example - [x] Write diagram example - [ ] Do something else  renders as\n Write math example Write diagram example Do something else  Tables Represent your data in tables:\n| First Header | Second Header | | ------------- | ------------- | | Content Cell | Content Cell | | Content Cell | Content Cell |  renders as\n   First Header Second Header     Content Cell Content Cell   Content Cell Content Cell    Asides Academic supports a shortcode for asides, also referred to as notices, hints, or alerts. By wrapping a paragraph in {{% alert note %}} ... {{% /alert %}}, it will render as an aside.\n{{% alert note %}} A Markdown aside is useful for displaying notices, hints, or definitions to your readers. {{% /alert %}}  renders as\n A Markdown aside is useful for displaying notices, hints, or definitions to your readers.   Spoilers Add a spoiler to a page to reveal text, such as an answer to a question, after a button is clicked.\n{{\u0026lt; spoiler text=\u0026quot;Click to view the spoiler\u0026quot; \u0026gt;}} You found me! {{\u0026lt; /spoiler \u0026gt;}}  renders as\n Click to view the spoiler  You found me!    Icons Academic enables you to use a wide range of icons from Font Awesome and Academicons in addition to emojis.\nHere are some examples using the icon shortcode to render icons:\n{{\u0026lt; icon name=\u0026quot;terminal\u0026quot; pack=\u0026quot;fas\u0026quot; \u0026gt;}} Terminal {{\u0026lt; icon name=\u0026quot;python\u0026quot; pack=\u0026quot;fab\u0026quot; \u0026gt;}} Python {{\u0026lt; icon name=\u0026quot;r-project\u0026quot; pack=\u0026quot;fab\u0026quot; \u0026gt;}} R  renders as\n  Terminal\n Python\n R\nDid you find this page helpful? Consider sharing it 🙌 ","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"07e02bccc368a192a0c76c44918396c3","permalink":"/post/writing-technical-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/post/writing-technical-content/","section":"post","summary":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.","tags":null,"title":"Writing technical content in Academic","type":"post"},{"authors":["Dylan Wiwad"],"categories":[],"content":"from IPython.core.display import Image Image('https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png')  print(\u0026quot;Welcome to Academic!\u0026quot;)  Welcome to Academic!  Install Python and JupyterLab  Install Anaconda which includes Python 3 and JupyterLab.\nAlternatively, install JupyterLab with pip3 install jupyterlab.\nCreate or upload a Jupyter notebook Run the following commands in your Terminal, substituting \u0026lt;MY-WEBSITE-FOLDER\u0026gt; and \u0026lt;SHORT-POST-TITLE\u0026gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:\nmkdir -p \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ cd \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ jupyter lab index.ipynb  The jupyter command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.\nEdit your post metadata The first cell of your Jupter notebook will contain your post metadata ( front matter).\nIn Jupter, choose Markdown as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:\n--- title: My post's title date: 2019-09-01 # Put any other Academic metadata here... ---  Edit the metadata of your post, using the documentation as a guide to the available options.\nTo set a featured image, place an image named featured into your post\u0026rsquo;s folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\nConvert notebook to Markdown jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=.  Example This post was created with Jupyter. The orginal files can be found at https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"Back in 2015, when I was a second year Master\u0026rsquo;s student I co-chaired a symposium at the annual Society for Personality and Social Psychology (SPSP) conference with Shai Davidai. In short, across three great talks given by Drs. Mike Norton and Shai Davidai, as well as myself, we covered:\n  High economic inequality is often justified by the belief in social mobility, the possibility that anyone can increase their economic standing through hard work. Three speakers discuss new research on subjective perceptions of inequality and social mobility, and the how these perceptions impact emotional well-being.\n  We were lucky enough to have videographers at SPSP film each of the talks and post them on youtube. So, I thought I would archive them here with their abstracts. Take a trip back to SPSP of (almost) four years ago (!!) and see some of the cutting edge work on economic inequality and mobility.\nDr. Mike Norton  Mike\u0026rsquo;s talk was titled \u0026ldquo;How Much (More) Should CEOs Make? A Universal Desire for More Equal Pay\u0026rdquo; and was based on this paper. Here is the abstract for the talk:\n  We assess people's preferred wage differentials between rich and poor, and determine whether these ideal ratios are commonly-held. Using survey data from 40 countries (N = 55,238), we compare respondents' estimates of the actual wages of chief executive officers and unskilled workers to their ideals for what those wages should be. We show that ideal pay gaps between CEOs and unskilled workers are significantly smaller than estimated pay gaps, and that there is consensus across countries, socioeconomic status, and political beliefs for ideal pay ratios. Moreover, data from 16 countries reveals that people dramatically underestimate actual pay inequality. In the United States the actual pay ratio of CEOs to unskilled workers (354:1) far exceeded the estimated ratio (30:1) which in turn far exceeded the ideal ratio (7:1). People underestimate pay gaps, and their ideal pay gaps are even further from reality than their erroneous underestimates.\n    Dr. Shai Davidai Shai\u0026rsquo;s talk was titled \u0026ldquo;Building a More Mobile America – One Income Quintile at a Time\u0026rdquo; and was based on this paper. Here is the abstract for the talk:\n  A core tenet of the American ethos is that there is considerable economic mobility. Americans seem willing to accept vast financial inequalities as long as they believe that everyone has the opportunity to succeed. We examined whether people’s beliefs about the amount of economic mobility in the United States conform to reality. In a nationally representative sample (N=3,034), we found that: (1) people believe there is more upward mobility than downward mobility, (2) people overestimate the amount of upward mobility and underestimate the amount of downward mobility and (3) poorer individuals believe there is more mobility than richer ones. An additional study (N=290) replicated these results and found that political affiliation influences perceptions of mobility, with conservatives believing that the economic system is more dynamic than liberals do. We discuss these findings in terms of system justification theory and consider the implications for contemporary political debates in the United States.\n    Myself My talk was titled \u0026ldquo;Belief in high social mobility and emotional well-being\u0026rdquo; and was based on the work that became my Master\u0026rsquo;s Thesis. Here is the abstract for the talk:\n  The American Dream posits that anyone can move between income levels, but recent reports document that income mobility is at an all-time low (Chetty, Hendren, Kline, \u0026amp; Saez, 2013). High levels of income mobility may offer economic advantages, but does perceived mobility impact well-being? Past research provides conflicting hypotheses, suggesting both positive and negative well-being outcomes (Diener, Lucas, \u0026amp; Oishi, 2002; Smith, Loewenstein, Jankovic, \u0026amp; Ubel, 2009). In Study 1(n=100) participants who believed they had higher income mobility reported higher positive affect and life satisfaction. In Study 2 (n=456) participants randomly assigned to read about high (vs. low) income mobility reported higher positive, and lower negative, affect. In Study 3 (n=435) we replicated Study 2 in a nationally representative sample. Across all three studies emotional benefits persisted regardless of the participants’ income level. These findings suggest there are emotional benefits to perceiving high income mobility, regardless of current economic standing.\n    The End! And that\u0026rsquo;s it for our 2015 symposium! I hope you enjoyed! Please feel free to reach out if you have any questions!\nBonus: I chaired another symposium on inequality at SPSP 2017. An undergraduate student writer wrote a post about Shai\u0026rsquo;s talk on \u0026ldquo;The Great Gatsby Curve\u0026rdquo; here.\n","date":1539752400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539752400,"objectID":"492b20c7e5a05d199bacd1c8ab9a7f73","permalink":"/project/spsp_2015/","publishdate":"2018-10-17T00:00:00-05:00","relpermalink":"/project/spsp_2015/","section":"project","summary":"Videos of our three talks from SPSP 2015 on economic inequality and mobility","tags":["Inequality","Economic Mobility","Blog"],"title":"TBT Our SPSP 2015 Symposium on Economic Mobility and Inequality","type":"project"},{"authors":null,"categories":null,"content":"Predicting Car Prices with KNN Regression In this brief tutorial I am going to run through how to build, implement, and cross-validate a simple k-nearest neighbours (KNN) regression model. Simply put, KNN model is a variant of simple linear regression wherein we utilize information about neighbouring data points to predict an unknown outcome. To put a more concrete spin on this, and use the name quite literally, we can think about this in the context of your next door neighbours. If the outcome I wanted to predict was your personal income and didn\u0026rsquo;t have any other information about you, besides where you live, I might be wise to just ask your next door neighbours. I might decide to run a real life 4-nearest neighbours test. The two houses to the right of yours have a household incomes of $65,000 and $90,000 respectively, and the two to the left $100,000 and $72,000. In the simplest possible fashion I would then just assume you make somewhere in the ballpark of what your neighbours make, take an average, and predict that you have a household income of $81,750.\nThis is an extremely basic explanation of how KNN algorithms work. But how do we decide who someone\u0026rsquo;s \u0026ldquo;neighbours\u0026rdquo; are when we aren\u0026rsquo;t talking about literal neighbours? That is, how do we know what other observations are similar to our target? Enter: distance metrics. The most common distance metric in for knn regression problems is Euclidian distance. This isn\u0026rsquo;t meant to be an extremely math heavy tutorial, so suffice it to say that in running a knn model, for each and every observation in a data set where we want to predict an outcome we grab their k-nearest neighbours, as defined by a metric such as Euclidean distance, from the training set, look at their values for the main dependent variable, and predict the new value based on the neighbours (e.g., as an average in a regression problem, or a \u0026ldquo;majority vote\u0026rdquo; in a classification problem).\nKNN regression can be used for both classification (i.e., predicting a binary outcome) and regression (i.e., predicting a continuous outcome). The procedure for implementation is largely the same and in this post I\u0026rsquo;m going to focus on regression. Specifically, the question at hand is: can we predict how much a used car is going to sell for? For this question I am going to utilize a data set from the machine learning repository at The University of California, Irvine. First up is to just take a brief look at what is actually in the dataset.\nBrief Exploratory Analysis and Cleaning These data contain a ton of information on a lot of different cars. For the purposes of this tutorial, I\u0026rsquo;m just going to pull out a set of relevant columns and work with those.\nimport pandas as pd # Need to specify the headers for this dataset cols = [\u0026quot;symboling\u0026quot;, \u0026quot;normalized_losses\u0026quot;, \u0026quot;make\u0026quot;, \u0026quot;fuel_type\u0026quot;, \u0026quot;aspiration\u0026quot;, \u0026quot;num_doors\u0026quot;, \u0026quot;body_style\u0026quot;, \u0026quot;drive_wheels\u0026quot;, \u0026quot;engine_location\u0026quot;, \u0026quot;wheel_base\u0026quot;, \u0026quot;length\u0026quot;, \u0026quot;width\u0026quot;, \u0026quot;height\u0026quot;, \u0026quot;curb_weight\u0026quot;, \u0026quot;engine_type\u0026quot;, \u0026quot;num_cylinders\u0026quot;, \u0026quot;engine_size\u0026quot;, \u0026quot;fuel_system\u0026quot;, \u0026quot;bore\u0026quot;, \u0026quot;stroke\u0026quot;, \u0026quot;compression_ratio\u0026quot;, \u0026quot;horsepower\u0026quot;, \u0026quot;peak_rpm\u0026quot;, \u0026quot;city_mpg\u0026quot;, \u0026quot;highway_mpg\u0026quot;, \u0026quot;price\u0026quot;] cars = pd.read_csv(\u0026quot;imports-85.data\u0026quot;, names=cols) cars.dtypes  symboling int64 normalized_losses object make object fuel_type object aspiration object num_doors object body_style object drive_wheels object engine_location object wheel_base float64 length float64 width float64 height float64 curb_weight int64 engine_type object num_cylinders object engine_size int64 fuel_system object bore object stroke object compression_ratio float64 horsepower object peak_rpm object city_mpg int64 highway_mpg int64 price object dtype: object  So, as you can see we\u0026rsquo;ve got 25 columns that might be informative in predicting a car\u0026rsquo;s sale price, ranging from both highway and city miles per gallon to the number of doors a car has. Let\u0026rsquo;s take a quick look at the first few rows of the data just so we can get a sense of how it actually looks.\ncars.head()     symboling normalized_losses make fuel_type ... horsepower peak_rpm city_mpg highway_mpg price     0 3 ? alfa-romero gas ... 111 5000 21 27 13495   1 3 ? alfa-romero gas ... 111 5000 21 27 16500   2 1 ? alfa-romero gas ... 154 5000 19 26 16500   3 2 164 audi gas ... 102 5500 24 30 13950   4 2 164 audi gas ... 115 5500 18 22 17450    5 rows × 26 columns\n To keep things simple and focus just on numeric columns without much feature engineering for now, it seems like we can use wheelbase, length, width, height, engine size, compression ratio, and city/highway mpr to predict price. Some of these predictors probably offer more information that others (miles per gallon is probably more informative of a car\u0026rsquo;s sale price than curb weight).\nWe\u0026rsquo;re probably going to want to use some of the other variables that aren\u0026rsquo;t numeric - they are likely also meaningful. So, right now we\u0026rsquo;ll deal with counting and seeing where our missing values are, as well as turning relevant columns numeric so we can actually use them. You will have noticed above that there were some questionmarks in the data - we just need to turn those into missing values. So I do this in the code below, then I select sex non-numeric columns that might be meaningful (normalized_losses, bore, stroke, horsepower, peak_rpm, and price) and turn them numeric. These ones are easy as they are actually numbers, they just are currently stored as objects.\nimport numpy as np cars = cars.replace('?', np.nan) # Now lets make things numeric num_vars = ['normalized_losses', \u0026quot;bore\u0026quot;, \u0026quot;stroke\u0026quot;, \u0026quot;horsepower\u0026quot;, \u0026quot;peak_rpm\u0026quot;, \u0026quot;price\u0026quot;] for i in num_vars: cars[i] = cars[i].astype('float64') cars.head()     symboling normalized_losses make fuel_type ... horsepower peak_rpm city_mpg highway_mpg price     0 3 NaN alfa-romero gas ... 111.0 5000.0 21 27 13495.0   1 3 NaN alfa-romero gas ... 111.0 5000.0 21 27 16500.0   2 1 NaN alfa-romero gas ... 154.0 5000.0 19 26 16500.0   3 2 164.0 audi gas ... 102.0 5500.0 24 30 13950.0   4 2 164.0 audi gas ... 115.0 5500.0 18 22 17450.0    5 rows × 26 columns\n Everything looks good now - how many missing values do we have in the normalized losses column?\nprint(\u0026quot;normalized losses: \u0026quot;, cars['normalized_losses'].isnull().sum())  normalized losses: 41  There are 41 missing values in the normalized_losses column. Given there are only 205 rows, thats a decent chunk missing. I\u0026rsquo;m not sure this column is the most useful, so we\u0026rsquo;ll just not use this column in our analyses at all. Let\u0026rsquo;s take a look at our other numeric columns and see what the missing values are like. The below chunk just calculates the sum of missing values for each variable and displays that sum.\ncars.isnull().sum()  symboling 0 normalized_losses 41 make 0 fuel_type 0 aspiration 0 num_doors 2 body_style 0 drive_wheels 0 engine_location 0 wheel_base 0 length 0 width 0 height 0 curb_weight 0 engine_type 0 num_cylinders 0 engine_size 0 fuel_system 0 bore 4 stroke 4 compression_ratio 0 horsepower 2 peak_rpm 2 city_mpg 0 highway_mpg 0 price 4 dtype: int64  So it looks like most of our columns are pretty good, with only a couple missing values here and there. The most crucial one here is price, our dependent variable; there are four cars that don\u0026rsquo;t have prices. Given that the number of missing rows is, at most, about 2%, I\u0026rsquo;m just going to listwise delete any row that has a missing variable in any of these. I don\u0026rsquo;t like mean imputation as it is purely making up data.\nI\u0026rsquo;ll start with the price column because its the most important and I suspect the rows that are missing price are the same rows missing the other data as well. Here I just drop any rows that are missing price data:\ncars = cars.dropna(subset = ['price'])  Now lets check the missing values again, just to be sure it worked correctly:\ncars.isnull().sum()  symboling 0 normalized_losses 37 make 0 fuel_type 0 aspiration 0 num_doors 2 body_style 0 drive_wheels 0 engine_location 0 wheel_base 0 length 0 width 0 height 0 curb_weight 0 engine_type 0 num_cylinders 0 engine_size 0 fuel_system 0 bore 4 stroke 4 compression_ratio 0 horsepower 2 peak_rpm 2 city_mpg 0 highway_mpg 0 price 0 dtype: int64  Now, I\u0026rsquo;ll do the same to listwise delete the other numeric columns.\ncars = cars.dropna(subset = ['bore', 'stroke', 'horsepower', 'peak_rpm'])  Now, we should have no missing data and be ready to go! The next step is to convert all the numeric columns into standardized z-scores. This is especially important if your variables are on drastically different scales. For instance here, horsepower is generally way up over 100 and miles per gallon is never more than about 45. So what I\u0026rsquo;ll do below is trim the dataset down just to the numeric columns, and then convert each of those columns into a z-score. Then, I save this into a new dataset called \u0026ldquo;normalized.\u0026rdquo; This is generally good practice because that way we retain our original dataset in case we need to go back to it.\ncols = ['wheel_base', 'length', 'width', 'height', 'curb_weight', 'engine_size', 'bore', 'stroke', 'horsepower', 'peak_rpm', 'city_mpg', 'highway_mpg', 'price'] cars = cars[cols] normalized_cars = (cars - cars.mean()) / (cars.std())  Modeling Alright, onward into some modeling! We\u0026rsquo;ve got a nice clean data set full of numeric columns. The first thing I\u0026rsquo;m going to do is create a couple of univariate (i.e., just one predictor) models, just to see how informative certain predictors are. Now, in more traditionally academic regression contexts this would be akin to just running some linear regressions with individual predictors. For example, we might see how well highway miles per gallon \u0026ldquo;predicts\u0026rdquo; sale price on it\u0026rsquo;s own. Of course, in the academic context, when we say predict what we actually mean is \u0026ldquo;variance explained\u0026rdquo; - we\u0026rsquo;re really finding out how much of the variance in sale price can be explained just by looking at highway miles per gallon.\nIn the machine learning context, we are actually more concerned with prediction. That is, if we build a KNN model, where we were only identifying neighbours based on how similar they were in highway miles per gallon, could we accurately predict price? There are myriad different ways we could judge accuracy, but here I\u0026rsquo;m going to use Root Mean Squared Error (RMSE). RMSE is one of the most common error metrics for regression based machine learning. Again, this is not meant to be a math-heavy tutorial so I won\u0026rsquo;t go into it deeply here but it quantifies how far off our predictions were from the actual values.\nSo, to start running some very basic univariate KNN models I imported two pieces of the sklearn package below, one for training a KNN model (KNeighborsRegressor) and one for calculating the mean squared error (mean_squared_error), from which we will derive the RMSE. Now, we need to do a couple things to build these models. Specifically, we need to choose the predictor we want to test, choose the dependent variable, and split our data into training and test sets so we reduce the risk of overfitting.\nTo accomplish this, I define a function that takes in three arguments: (1) our training column(s), (2), our target column, and (3) the dataset to use. Using this information, the function first instatiates an instance of a k-nearest neighbours regression (stored as \u0026ldquo;knn\u0026rdquo;) and sets a set so our results are reproducible. Next, the function shuffles the data into a random order, splits the data in half, designates the top half as the training data, and the bottom half as the test data.\nThen, we get down to the nitty gritty. The function fits the knn object on the specified training and test columns of the training data, uses that model to make predictions on the test data, and then calculates the RMSE (e.g., the difference between the predictions our model made for each car in the test set\u0026rsquo;s price and the actual prices).\nAs we move through, we\u0026rsquo;ll complicate this function bit by bit, adding extra stuff to it.\n# Writing a simple function that trains and tests univariate models # This function takes in three arguments: the predictor, the outcome, \u0026amp; the data from sklearn.neighbors import KNeighborsRegressor from sklearn.metrics import mean_squared_error def knn_train_test(train_col, target_col, df): knn = KNeighborsRegressor() np.random.seed(1) # Randomize order of rows in data frame. shuffled_index = np.random.permutation(df.index) rand_df = df.reindex(shuffled_index) # Divide number of rows in half and round. last_train_row = int(len(rand_df) / 2) # Select the first half and set as training set. # Select the second half and set as test set. train_df = rand_df.iloc[0:last_train_row] test_df = rand_df.iloc[last_train_row:] # Fit a KNN model using default k value. knn.fit(train_df[[train_col]], train_df[target_col]) # Make predictions using model. predicted_labels = knn.predict(test_df[[train_col]]) # Calculate and return RMSE. mse = mean_squared_error(test_df[target_col], predicted_labels) rmse = np.sqrt(mse) return rmse  Now that we\u0026rsquo;ve got this function defined, let\u0026rsquo;s use it! If you recall, I said I was going to just test some basic univariate models. So, I\u0026rsquo;m going to run our new function five times, getting the RMSE of five different predictors. I just chose four that I thought would be relatively meaningful in predicting price (city and highway miles per gallon, engine size, and horsepower) and one to serve as a logic check (width) - why would width predict the price of a car, unless larger vehicles are more expensive. In any case, my intuition suggests that width should be the word predictor of price.\n# Lets test a couple of predictors print('city mpg: ', knn_train_test('city_mpg', 'price', normalized_cars)) print('width: ', knn_train_test('width', 'price', normalized_cars)) print('highway mpg: ', knn_train_test('highway_mpg', 'price', normalized_cars)) print('engine size: ', knn_train_test('engine_size', 'price', normalized_cars)) print('horsepower: ', knn_train_test('horsepower', 'price', normalized_cars))  city mpg: 0.598975486019 width: 0.671608148246 highway mpg: 0.537913994132 engine size: 0.536691465842 horsepower: 0.571585852136  As I suspected, width is by quite a large margin the worst predictor of a car\u0026rsquo;s price. Of the couple predictors that I threw in there to test, the best most informative for determining a vehicles price seems to be it\u0026rsquo;s highway. So, if we wanted to be as accurate as possible while only using on predictor, we would want to use fuel economy on the highway.\n\u0026ldquo;Hyperparamaterization\u0026rdquo; If you recall, in KNN regression, we can set k to be whatever we want: 3, 5, 7, 100, 1000. Common values of k range from 3 to 10 - does tweaking our k value, and grabbing more or less neighbours to make our price guess, make the model fit better? As a test of this question, I\u0026rsquo;m going to modify the above function to take another argument: a k value. Then, I\u0026rsquo;ll test each of the five predictors above (plus some more) each with five different values of k (1, 3, 5, 7, and 9). This will effectively run our regression 25 times; city mpg with 1 neighbour, city mpg with 2 neighbours, and so on.\nThe way I\u0026rsquo;ve done this is to insert a list of k-values into the middle of the function, and set up an empty dictionary to store all our RMSEs. Then, I nested the code from previously that fits the model, generates our predictions, and calculates the RSME into a for loop that does this for each value of k. Lastly, it appends the RMSE to the dictionary and returns it.\nLastly, I specified a list of all the columns I want to build univariate models for, use a for loop to run the function on each of those columns, and append the results to another dictionary called \u0026ldquo;k_rmse_results.\u0026rdquo; Printing this dictionary gives us the name of the predictor, the specified k, and then the RMSE.\ndef knn_train_test_new(train_col, target_col, df): np.random.seed(1) # Randomize order of rows in data frame. shuffled_index = np.random.permutation(df.index) rand_df = df.reindex(shuffled_index) # Divide number of rows in half and round. last_train_row = int(len(rand_df) / 2) # Select the first half and set as training set. # Select the second half and set as test set. train_df = rand_df.iloc[0:last_train_row] test_df = rand_df.iloc[last_train_row:] k_values = [1,3,5,7,9] k_rmses = {} for k in k_values: # Fit model using k nearest neighbors. knn = KNeighborsRegressor(n_neighbors=k) knn.fit(train_df[[train_col]], train_df[target_col]) # Make predictions using model. predicted_labels = knn.predict(test_df[[train_col]]) # Calculate and return RMSE. mse = mean_squared_error(test_df[target_col], predicted_labels) rmse = np.sqrt(mse) k_rmses[k] = rmse return k_rmses k_rmse_results = {} # For each column from above, train a model, return RMSE value # and add to the dictionary `rmse_results`. variables = ['wheel_base', 'length', 'width', 'height', 'curb_weight', 'engine_size', 'bore', 'stroke', 'horsepower', 'peak_rpm', 'city_mpg', 'highway_mpg'] for var in variables: rmse_val = knn_train_test_new(var, 'price', normalized_cars) k_rmse_results[var] = rmse_val k_rmse_results  {'bore': {1: 1.2142304178718561, 3: 0.86766581048215152, 5: 0.89458788943880752, 7: 0.94676716177240661, 9: 0.95385344053196963}, 'city_mpg': {1: 0.69529747854104784, 3: 0.59031417913396289, 5: 0.59897548601904338, 7: 0.59715938629269016, 9: 0.57728649652220132}, 'curb_weight': {1: 0.8365387787670262, 3: 0.64395375801733934, 5: 0.57031290606074236, 7: 0.51644149986604171, 9: 0.51839468763038343}, 'engine_size': {1: 0.55456842477058543, 3: 0.54125650939355474, 5: 0.53669146584152094, 7: 0.51899873944760311, 9: 0.50362821678292591}, 'height': {1: 1.1521894508998922, 3: 1.0168354498989998, 5: 0.94018342361170537, 7: 0.98192402693779424, 9: 0.94562106614391528}, 'highway_mpg': {1: 0.69402405950428248, 3: 0.57416390399142236, 5: 0.53791399413219376, 7: 0.53495996840130122, 9: 0.54899088943686292}, 'horsepower': {1: 0.54931712010379319, 3: 0.58337889657418729, 5: 0.57158585213578872, 7: 0.59314672824644243, 9: 0.59002690698940097}, 'length': {1: 0.65713817747103875, 3: 0.63950652453727652, 5: 0.64700844560860649, 7: 0.66892394999830362, 9: 0.6572441270128111}, 'peak_rpm': {1: 0.85899385207057188, 3: 0.88634665981443039, 5: 0.90984280257609562, 7: 0.90757986581712302, 9: 0.90022237747155309}, 'stroke': {1: 0.91796375714948086, 3: 0.88113492952413897, 5: 0.89586641314603555, 7: 0.94574853212680499, 9: 0.92121730389055356}, 'wheel_base': {1: 0.70984063675235898, 3: 0.70981264790591259, 5: 0.70662378566203043, 7: 0.71073380297018984, 9: 0.72499494497288963}, 'width': {1: 0.77543242861316763, 3: 0.68812005931548847, 5: 0.67160814824580428, 7: 0.5826291501627695, 9: 0.56790774193128279}}  Generally speaking, just eyeballing within each predictor, going up to k = 9 worked the best for some of the models, but not for others. The error actually went up as we increased k for some. With a k of 9, the best univariate predictor seems to be engine size, with predictions only off by about half a standard deviation. So thats the best so far, but it\u0026rsquo;s still not great. When trying to predict an outcome, univariate models aren\u0026rsquo;t going to be extremely informative. Outcomes are complex, so let\u0026rsquo;s build a model that reflects that and takes in more than one column.\nWhat I\u0026rsquo;ve done to accomplish this is modified the above function to take multiple columns and then train and test some models using the the best two, then three, four, and five columns from above. By eyeballing at the 5-nearest neighbours level, the columns that predict the best in isolation are engine size, highway mpg, curb_weight, horsepower, and city mpg.\ndef knn_train_test_mult(train_cols, target_col, df): np.random.seed(1) # Randomize order of rows in data frame. shuffled_index = np.random.permutation(df.index) rand_df = df.reindex(shuffled_index) # Divide number of rows in half and round. last_train_row = int(len(rand_df) / 2) # Select the first half and set as training set. # Select the second half and set as test set. train_df = rand_df.iloc[0:last_train_row] test_df = rand_df.iloc[last_train_row:] k_values = [5] k_rmses = {} for k in k_values: # Fit model using k nearest neighbors. knn = KNeighborsRegressor(n_neighbors=k) knn.fit(train_df[train_cols], train_df[target_col]) # Make predictions using model. predicted_labels = knn.predict(test_df[train_cols]) # Calculate and return RMSE. mse = mean_squared_error(test_df[target_col], predicted_labels) rmse = np.sqrt(mse) k_rmses[k] = rmse return k_rmses train_cols_2 = ['engine_size', 'highway_mpg'] train_cols_3 = ['engine_size', 'highway_mpg', 'curb_weight'] train_cols_4 = ['engine_size', 'highway_mpg', 'curb_weight', 'horsepower'] train_cols_5 = ['engine_size', 'highway_mpg', 'curb_weight', 'horsepower', 'city_mpg'] k_rmse_results = {} rmse_val = knn_train_test_mult(train_cols_2, 'price', normalized_cars) k_rmse_results[\u0026quot;two best features\u0026quot;] = rmse_val rmse_val = knn_train_test_mult(train_cols_3, 'price', normalized_cars) k_rmse_results[\u0026quot;three best features\u0026quot;] = rmse_val rmse_val = knn_train_test_mult(train_cols_4, 'price', normalized_cars) k_rmse_results[\u0026quot;four best features\u0026quot;] = rmse_val rmse_val = knn_train_test_mult(train_cols_5, 'price', normalized_cars) k_rmse_results[\u0026quot;five best features\u0026quot;] = rmse_val k_rmse_results  {'five best features': {5: 0.49634970383078636}, 'four best features': {5: 0.48157162981314988}, 'three best features': {5: 0.50707231602531166}, 'two best features': {5: 0.43695579560820619}}  So, what we have here is four new models that combine the best features we identified above. For example, the two best features model is using engine size and highway mpg to predict sale price with k set to 5. The three best predictors model is using engine size, highway mpg, and curb weight. Interestingly, adding more predictors does not increase the performance of the model at all - in fact, it makes it worse. This is not at all surprising because we started with the two best predictors, and just tacked on worse and worse predictors. So simply adding more doesn\u0026rsquo;t do anything good for us, it just adds muddier predictors to the model.\nThe best performing model here to predict the price of a car is simply it\u0026rsquo;s engine size and highway miles per gallon. Let\u0026rsquo;s combine the two tests we\u0026rsquo;ve done so far and go a little overboard with it. Let\u0026rsquo;s test are four multivariate models at all ks ranging from 1 to 25.\ndef knn_train_test_mult(train_cols, target_col, df): np.random.seed(1) # Randomize order of rows in data frame. shuffled_index = np.random.permutation(df.index) rand_df = df.reindex(shuffled_index) # Divide number of rows in half and round. last_train_row = int(len(rand_df) / 2) # Select the first half and set as training set. # Select the second half and set as test set. train_df = rand_df.iloc[0:last_train_row] test_df = rand_df.iloc[last_train_row:] k_values = list(range(1,25)) k_rmses = {} for k in k_values: # Fit model using k nearest neighbors. knn = KNeighborsRegressor(n_neighbors=k) knn.fit(train_df[train_cols], train_df[target_col]) # Make predictions using model. predicted_labels = knn.predict(test_df[train_cols]) # Calculate and return RMSE. mse = mean_squared_error(test_df[target_col], predicted_labels) rmse = np.sqrt(mse) k_rmses[k] = rmse return k_rmses k_rmse_results_2 = {} rmse_val = knn_train_test_mult(train_cols_2, 'price', normalized_cars) k_rmse_results_2[\u0026quot;two best features\u0026quot;] = rmse_val rmse_val = knn_train_test_mult(train_cols_3, 'price', normalized_cars) k_rmse_results_2[\u0026quot;three best features\u0026quot;] = rmse_val rmse_val = knn_train_test_mult(train_cols_4, 'price', normalized_cars) k_rmse_results_2[\u0026quot;four best features\u0026quot;] = rmse_val rmse_val = knn_train_test_mult(train_cols_5, 'price', normalized_cars) k_rmse_results_2[\u0026quot;five best features\u0026quot;] = rmse_val k_rmse_results_2  {'five best features': {1: 0.48370851070283638, 2: 0.45709593736092696, 3: 0.45932846135166017, 4: 0.47702516372024495, 5: 0.49634970383078636, 6: 0.51326367838750764, 7: 0.49909158266517145, 8: 0.49234720868686166, 9: 0.49925986215124729, 10: 0.50320132862625788, 11: 0.49352187011331128, 12: 0.50969596379759741, 13: 0.51666729281793589, 14: 0.52430330537451841, 15: 0.52831175797337293, 16: 0.53372729477563852, 17: 0.53629514907736431, 18: 0.53771760713738892, 19: 0.54366780131360726, 20: 0.55390626006954025, 21: 0.56186625263562961, 22: 0.56399228907515964, 23: 0.56686293421110434, 24: 0.56749133395228601}, 'four best features': {1: 0.46011695087263338, 2: 0.47254209382853879, 3: 0.4957675847479055, 4: 0.47457606714743178, 5: 0.48157162981314988, 6: 0.4988487989184765, 7: 0.51321993316267744, 8: 0.51485477394479984, 9: 0.51767653423898286, 10: 0.5323478616627898, 11: 0.51621784291210482, 12: 0.51180481420952084, 13: 0.52093660357028182, 14: 0.5264585716201885, 15: 0.53375627848681184, 16: 0.53867876043121632, 17: 0.539751445854185, 18: 0.54052905274747964, 19: 0.54394381259000146, 20: 0.55263828123323866, 21: 0.55895871633176053, 22: 0.56273074037859749, 23: 0.55965116497813094, 24: 0.56079209060485291}, 'three best features': {1: 0.53304672541629627, 2: 0.47236953984714419, 3: 0.48544239785342919, 4: 0.46468465687861726, 5: 0.50707231602531166, 6: 0.50672338800144279, 7: 0.52042141176821866, 8: 0.51959755832321974, 9: 0.51531096459267978, 10: 0.52080169867439063, 11: 0.52322240088596494, 12: 0.52075280982124794, 13: 0.51183217030638006, 14: 0.51868070682253542, 15: 0.52774088364145721, 16: 0.53035523273810814, 17: 0.53183552745587126, 18: 0.53451532836203997, 19: 0.54155167597224763, 20: 0.54551779898598018, 21: 0.54674295619522062, 22: 0.54707824469373634, 23: 0.54985793303062935, 24: 0.55512440029648535}, 'two best features': {1: 0.4962211708123152, 2: 0.42439666654800412, 3: 0.37244955551446796, 4: 0.38221587546513652, 5: 0.43695579560820619, 6: 0.49363489340281513, 7: 0.50823941279743867, 8: 0.51418965195989808, 9: 0.52616341995960625, 10: 0.53013621032483371, 11: 0.53671709358748898, 12: 0.53444184258976579, 13: 0.51587142155192167, 14: 0.51205713655316698, 15: 0.5119686970820041, 16: 0.51750488428455055, 17: 0.52218955380387977, 18: 0.53175784405886029, 19: 0.54377416147308744, 20: 0.54302569583633942, 21: 0.54968364588915308, 22: 0.5508362135961884, 23: 0.55132842211110611, 24: 0.55665088230669157}}  As you can probably tell, it\u0026rsquo;s hard to make sense of this. For our different models with 2, 3, 4, and 5 predictors, what is the most accurate value of k? The best way to explore this might be just to plot it out! So, I\u0026rsquo;ll take the big dictionary that\u0026rsquo;s printed above and plot it out with RMSE on the y axis, k on the x axis, and then each colored line is a different model.\nimport matplotlib.pyplot as plt % matplotlib inline for k,v in k_rmse_results_2.items(): x = list(v.keys()) y = list(v.values()) plt.plot(x,y) plt.xlabel('k value') plt.ylabel('RMSE') plt.show()   The pattern you can discern just looking at the numbers becomes clear here when looking at the graph. The best models for any given set of predictors is right around a k of 3-5. Once we start adding more than that, the performance of the model gets worse.\nConclusion So, we\u0026rsquo;ve learned a lot about how to run a KNN regression using python, how to tweak paramaters like the number of predictors in a model and the number of neighbors we use, and how to evaluate those models.\nFrom this, we learned in our simple set of predictions that the most accurate model is using just the two best predictors, engine size and highway miles per gallon, with k set somewhere in the 3-5 range.\nI hope you found this tutorial helpful, and I encourage you to give it a shot for yourself using this dataset, or any other, from UCI\u0026rsquo;s rich machine learning repository!\n","date":1538888400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538888400,"objectID":"f8cbd34c2280ac247d67fc3112a8457a","permalink":"/project/car_prices/","publishdate":"2018-10-07T00:00:00-05:00","relpermalink":"/project/car_prices/","section":"project","summary":" A basic introduction to KNN regression for machine learning","tags":["Python","sklearn","machine-learning","knn regression"],"title":"Maching Learning Tutorial: Predicting Car Prices with K-Nearest Neighbours (KNN) Regression","type":"project"},{"authors":null,"categories":null,"content":"Myriad past research demonstrates a fairly strong link between money and happiness. That is, people are generally happier when they make more money up to around the $75,000 a year mark (e.g., Kahneman \u0026amp; Deaton, 2010). However, one possible threat to this relationship is the level of economic inequality. Really, one could imagine this relationship going in either directions. On one hand, it seems intuitive that money has more happiness purchasing-power when your immediate community is highly unequal. In this case, having a lot of money could make you more satisfied to have \u0026lsquo;made it.\u0026rsquo; On the other hand, it could be extremely uncomfortable to be on the top of the economic ladder when poverty is so readily apparent.\nI tested this question with a set of multilevel models using four different datasets, which can be found here:\nsurvey_data.csv; This dataset contains 1,441 survey responses from two qualtrics national panels. The key individual variables here are happiness, economic quintile, age, political ideology, and location (latitude and longitude). We collected these data in our lab as part of larger projects exploring the psychological correlates of perceived economic mobility.\nACS_14_5YR_B19083; this dataset is from the United States census and contains two county identifiers (FIPS code and county name), income inequality (Gini) for each county, and the standard error for each Gini coefficient.\ngini.by.state; this dataset is also from the United States census and contains two state identifiers (FIPS code and state name), income inequality (Gini) for each state, and the standard error for each Gini coefficient.\nmobility.by.county; this dataset is from the Harvard Mobility Project and contains a measure of income mobility, as well as various population demographics, for each county. The measure I will be using, absolute upward mobility, quantifies the average income percentile for a child whose parents were in the 25th percentile. So, for example, if a county has an absolute upward mobility value of 40 this means that the children of parents who were in the 25th percentile of the income distribution ended up, on average, in the 40th percentile.\nSo, some of the data was data we had collected in the past and some of the data came from established sources. The first thing I did was take a quick look at where our participants were in the United States.\n So, we can see that the participants are pretty spread out across the United States, with a bit of a dearth in the central United States; most participants seem to be in the Eastern U.S. I used this geolocation data to assign each participant the county in which they live. I will spare you the nitty-gritty details of the multilevel modeling, but if you are so inclined you can find everything here.\nWhat I found was that, in line with past research, personal income (as measured by which income quintile the person reports being in) was a very strong predictor of happiness. In accounting for all the individual (age, gender, political ideology) and county-level (inequality) factors, it income was the only significant determinant of happiness. While I didn\u0026rsquo;t observe a significant interaction between income and happiness, suggesting that there could be a relationship here. But what does that relationship actually look like?\nIn order to take a look at this, I did a quartile split on income inequality and binned participants into four categories: extremely low inequality (gini less than .43), low inequality (gini between .43 and .45), high inequality (gini more than .45 and less than.48), and very high inequality (gini more than .48). Then, I plotted out the simple relationshp between income quintile and happiness in each of these inequality categories.\n What you can see here pretty clearly is that the slope of the relationship in counties with less inequality (the green and orange lines) is steeper than in counties with more inequality (the purple and pink lines). That is to say, money seems to have more \u0026ldquo;happiness purchasing power\u0026rdquo; when people live in more equal communities. Just to look at this in another way, here are the data plotted as actual means instead of over simplified regression lines.\n The above presented data show that the increase we get in happiness from having more money is diminished in places with higher inequality. One possible reason for this is simply exposure to inequality through poverty. Looking only at the people who reported being in the fifth quintile, you can see that happiness is the lowest in the high and extremely high inequality places. It’s possible that in these places the high income people are exposed to more poverty, and thus feel an increased sense of wealth guilt, leading to a dampened general happiness.\nThis is only one possible explanation and uncovering the mechanism here requires significant further exploration. For now, I’m going to just explore the data again, this time looking at the level of absolute upward mobility present in a county, instead of inequality.\nIncome and Happiness by County Upward Mobility One might suspect an interaction here such that the relationship between income and happiness is stronger in places with low mobility, perhaps as a sort of dissonance mechanism. That is, when one cannot move up the income ladder they rationalize and are thus happier with their level of income, regardless. Specifically, I would think that high income people are equally happy regardless of the level of mobility, but as mobility drops the baseline level of happiness for those in lower quintiles rises. Same with the previous analysis, I\u0026rsquo;m going to spare you all the nitty-gritty modeling details here and just present the main trends.\n This actually doesnt look all that different from the inequality version of the graph, but here there is no interaction whatsoever. The slopes are all equal. Let’s take a quick look at the graph of the actual data, instead of the regression lines:\n Looking at this graph versus the graph with county inequality makes it clear there really is no interaction here, just as the MLM data show. These models suggest that the relationship between income and happiness may indeed be influenced by the level of inequality present in the county one lives. Specifically, income has greater happiness purchasing power when you live in a more equal society. Perhaps this is due to the decreased availability of visible poverty and inequality, leading to a lower sense of wealth guilt among those living in the higher quintiles. On the other hand, it is also possible that in areas with lower inequality do not suffer so much from the middle class being washed out, thus having a higher income means one’s purchasing power is higher and can live relatively better off compared with someone of the same income bracket in a high-inequality city, where their money potentiall has less purchasing power.\nConclusion In sum, across this analysis I: (a) cleaned and combined four separate datasets into one useable dataset with individual and county level information, (b) ran a series of multilevel models exploring the interaction of county-level data and individual level relationships, and (C) unpacked these interactions with concise data visualization. Within the analyses, I first replicated a long-standing effect showing that higher wealth is related to higher happiness, overall. I then built upon this, showing that there appears to be a modest interaction between the level of inequality where one lives and the strength of the money-happiness relationship. Particularly, it appears that the happiness-purchasing power of money is greater when one lives in a more equal county. Lastly, I found that the level of absolute upward mobility in a county does not change the nature of the money-happiness relationship\n","date":1538715600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538715600,"objectID":"e1caa2b9243eacb6f6c2661bb61cbde9","permalink":"/project/mobility_happiness/","publishdate":"2018-10-05T00:00:00-05:00","relpermalink":"/project/mobility_happiness/","section":"project","summary":"Multilevel modeling exploring how county level inequality/mobility impacts the relationship between money and happiness","tags":["Data Viz","multilevel modeling","ggplot2"],"title":"Does High Economic Inequality, and Low Economic Mobility Threaten, the Relationship Between Income and Happiness?","type":"project"},{"authors":null,"categories":null,"content":"In this short post I\u0026rsquo;m going to do the first set of two visualizations exploring data regarding the grand tours that I pulled from Wikipedia. Specifically, here I\u0026rsquo;m going to look at a couple elements of the Tour de France over it\u0026rsquo;s entire 114 year history. Namely, I\u0026rsquo;m going to look at the length of the Tour, the average speed of the winner for each edition, their total winning time, and the margin that they won by.\nI\u0026rsquo;m not intending to do any inferential analyses, but what I\u0026rsquo;m expecting to see is that the Tour is getting faster and shorter, and is being won on smaller margins as time goes on. I\u0026rsquo;m going to hide all of the code in this particular notebook, but I will display some of the raw data and explain the process as I go along just so you can see where I started and where I got to, and then what those data actually look like. If you are interested in the more technical aspects of how I did all of these analyses, there is a seperate jupyter notebook in the repository I put up on github for this particular project here. These notebooks have more detailed information on what I did step by step, including all of the code.\nWith that, let\u0026rsquo;s get right into the data!\nTour de France Winners Data I grabbed these data from the wikipedia page for Tour GC winners, which can be found here on the wikipedia page for Tour de France GC Winners.\nLet\u0026rsquo;s take a quick look at these raw data; what does it look like once scraped from wikipedia and cleaned a little bit? Here\u0026rsquo;s the first and last five rows (i.e., the first and most recent five Tour winners):\n  We\u0026rsquo;ve got a lot of interesting information in this table, including the winner, their team, the number of stage wins, etc. However, given that I\u0026rsquo;m focusing here on speed of the tour, I\u0026rsquo;m going to pull out all the information I need and put it into one smaller new table. That is, I\u0026rsquo;ll pull out the columns for distance, time, and margin.\nFrom this, I\u0026rsquo;ll calculate a new column, average speed, which is simply the total distance divided by the winning time. Here\u0026rsquo;s what the new cleaned up data look like:\n  So here is the data that we\u0026rsquo;re going to work with from now on. We\u0026rsquo;ve got winning time in hours, total distance in kilometers, winning margin in seconds, and then average speed in kilometers per hour. Let\u0026rsquo;s take a look now, and see how things have changed over time.\n There\u0026rsquo;s tons of interesting things going on here! One thing to note first, is the blue bars correspond to missing data - as in, years where there was no Tour de France, or the Tour was structured differently. First, between 1905 and 1912 the Tour was scored on points so there is no distance and time data there. Second (and third), the Tour was not run during either of the World Wars. So, brushing over that let\u0026rsquo;s dive into the data.\nI\u0026rsquo;ll focus first on the top and bottom graphs, because most of the interesting stuff, in my opinion, is in the middle. First, the Tour has been getting shorter since right around WW I. This isn\u0026rsquo;t entirely surprising because back in the early 20th century the Tour was envisioned as a savage race for only the hardest men, where only one man would actually make it to the end. So, over the years the Tour has gotten shorter but still a formidable distance.\nSecond, the winning margin was huge back when the Tour was inhumanely difficult - the margin was in the realm of hours. However, since the 1950s the winning margin has been in the realm of minutes or seconds. We\u0026rsquo;ll dive a bit deeper into this later on.\nLastly, the overall winning time and average speed. This is where stuff gets a bit more interesting! The overall winning time has been getting lower and lower, which makes sense given the tour has gotten shorter and faster. The tour has been getting consistently faster over the years, even over just the last few decades. In fact, the average speed was still around 32kmh in the early 1970s and was over 40 kmh the last few years. Thats nearly a 20% increase in average speed over four decades. Average speed seemed to increase pretty sharply from the 1960s to the early 2000s, but has seemed pretty consistent since then. I did, however, notice some interesting blips in the 1990s and 2000s. Let\u0026rsquo;s dive a little bit deeper in the speed data for those years.\n When we zoom in on the last 46 years (1971-2017) we can see this pattern a little bit more clearly. When zoomed in, the pattern of the Tour getting faster looks a little bit less remarkable, but I think it\u0026rsquo;s still quite amazing when you unpack what these numbers are actually showing. I\u0026rsquo;m going to focus on the average speed column.\nWhile they don\u0026rsquo;t look like huge peaks, you\u0026rsquo;ll notice that the year of the Festina Affair and the last year of Lance (one year before Operacion Puerto) are the fastest edtions of the tour in the last half century - this is not over a trivial time frame. Even when you think about the advancements in bike, kit, and athlete training technology over the last decade, Lance in his last year was still faster than the current pros who have ten extra years of engineering underneath them.\nThe other bit, is that even though the slope of the average speed line doesn\u0026rsquo;t look crazy - it\u0026rsquo;s actually quite steep. The average speed of the Tour has increased by about 6 km/h since the 1970s, which is an increase of about 17%. All this while the tour has remained relatively consistent in its distance of about 3,500 km.\nIt would be nice to be able to factor in total elevation data (maybe they\u0026rsquo;re faster now because they\u0026rsquo;re climbing less), but I can\u0026rsquo;t find these data anywhere.\nThe general pattern seems to me to be twofold: (1) The tour is getting faster and faster and (2) There were relatively big drops in average speed after each major doping scandal, followed by slow and steady increases in speed (including over the last twelve years).\nThat being said, I don\u0026rsquo;t think the Tour will actually get substantially faster without drastic changes to the route or UCI rules. For instance, I can\u0026rsquo;t see the average speed hitting the mid-forties.\nLastly, I\u0026rsquo;m going to zoom in a bit on the winning margins.\n When we zoom in on the winning margins, we can see there is still a slight slope down. The time gap to the winner is getting smaller and smaller over time. Again, it doesn\u0026rsquo;t seem like much but the slope of this line goes down from ten minutes in 1971 (~ 600 seconds) to just 54 seconds in 2017. Again, I don\u0026rsquo;t think there\u0026rsquo;s really anywhere to go from here though. I suspect we will just continue to see the Tour being won on margins of less than a minute for the foreseeable future, unless there are major shakeups to the UCI\u0026rsquo;s rules.\nLimitations The biggest limitation of these basic visualizations, particularly the data about average speed, is that I would like to factor in the elevation gain for a given tour. The speed info is hard to interpret without it. For instance, a tour with 10 flat sprinters stages is likely to be faster, on average, than a tour with only 3. This doesn\u0026rsquo;t mean riders are getting faster overall, it just means the structure of the Tour was weighted towards faster stages. Unfortunately, I cannot find these data anywhere. I think we can assume though, that changes in the structure of the Tour don\u0026rsquo;t explain all changes in speed over the last few decades, where the race is very much a climbers race.\n","date":1461733200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461733200,"objectID":"4df3156c49d973ff78a13a6b570522b6","permalink":"/project/tdf_speed/","publishdate":"2016-04-27T00:00:00-05:00","relpermalink":"/project/tdf_speed/","section":"project","summary":"What does some basic web scraping teach us?","tags":["Data Viz","Python","Matplotlib","Web Scraping"],"title":"Visualizing the Tour De France","type":"project"},{"authors":["Dylan Wiwad","吳恩達"],"categories":["Demo","教程"],"content":"Create a free website with Academic using Markdown, Jupyter, or RStudio. Choose a beautiful color theme and build anything with the Page Builder - over 40 widgets, themes, and language packs included!\n Check out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\n 👉 Get Started 📚 View the documentation 💬 Ask a question on the forum 👥 Chat with the community 🐦 Twitter: @source_themes @GeorgeCushen #MadeWithAcademic 💡 Request a feature or report a bug ⬆️ Updating? View the Update Guide and Release Notes ❤️ Support development of Academic:  ☕️ Donate a coffee 💵 Become a backer on Patreon 🖼️ Decorate your laptop or journal with an Academic sticker 👕 Wear the T-shirt 👩‍💻 Contribute      Academic is mobile first with a responsive design to ensure that your site looks stunning on every device.   Key features:\n Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 15+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Themes Academic comes with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the sun/moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\n Choose a stunning theme and font for your site. Themes are fully customizable.\nEcosystem   Academic Admin: An admin tool to import publications from BibTeX or import assets for an offline site  Academic Scripts: Scripts to help migrate content to new versions of Academic  Install You can choose from one of the following four methods to install:\n  one-click install using your web browser (recommended)  install on your computer using Git with the Command Prompt/Terminal app  install on your computer by downloading the ZIP files  install on your computer with RStudio  Then personalize and deploy your new site.\nUpdating  View the Update Guide.\nFeel free to star the project on Github to help keep track of updates.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"/post/getting-started/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Create a beautifully simple website in under 10 minutes.","tags":["Academic","开源"],"title":"Academic: the website builder for Hugo","type":"post"},{"authors":null,"categories":["R"],"content":" R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 fit \u0026lt;- lm(dist ~ speed, data = cars) fit ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Coefficients: ## (Intercept) speed ## -17.579 3.932  Including Plots You can also embed plots. See Figure 1 for example:\npar(mar = c(0, 1, 0, 1)) pie( c(280, 60, 20), c(\u0026#39;Sky\u0026#39;, \u0026#39;Sunny side of pyramid\u0026#39;, \u0026#39;Shady side of pyramid\u0026#39;), col = c(\u0026#39;#0292D8\u0026#39;, \u0026#39;#F7EA39\u0026#39;, \u0026#39;#C4B632\u0026#39;), init.angle = -50, border = NA )  Figure 1: A fancy pie chart.   ","date":1437703994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1437703994,"objectID":"10065deaa3098b0da91b78b48d0efc71","permalink":"/post/2015-07-23-r-rmarkdown/","publishdate":"2015-07-23T21:13:14-05:00","relpermalink":"/post/2015-07-23-r-rmarkdown/","section":"post","summary":"R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.","tags":["R Markdown","plot","regression"],"title":"Hello R Markdown","type":"post"}]