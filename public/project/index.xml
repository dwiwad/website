<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | DYLAN WIWAD, Ph.D</title>
    <link>/project/</link>
      <atom:link href="/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 14 Dec 2020 00:00:00 -0600</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Projects</title>
      <link>/project/</link>
    </image>
    
    <item>
      <title>PsyPost Summary and Reddit Conversation on Wiwad et al., 2021, JESP</title>
      <link>/project/psypost_coverage_jesp/</link>
      <pubDate>Mon, 14 Dec 2020 00:00:00 -0600</pubDate>
      <guid>/project/psypost_coverage_jesp/</guid>
      <description>&lt;p&gt;Shortly after the publication of 
&lt;a href=&#34;/files/jesp_2021.pdf&#34;&gt;our paper&lt;/a&gt; in the &lt;em&gt;Journal of Experimental Social Psychology&lt;/em&gt; exploring how the COVID-19 pandemic has shifted attitudes towards poverty and inequality, Eric Dolan of 
&lt;a href=&#34;https://www.psypost.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PsyPost&lt;/a&gt; did a great 
&lt;a href=&#34;https://www.psypost.org/2020/11/study-suggests-the-covid-19-pandemic-has-altered-americans-attitudes-toward-inequality-and-the-poor-58598&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;write-up&lt;/a&gt; summarizing the main conclusions. If you don&amp;rsquo;t have time to read the actual paper, checking out his summary is a great way to get the gist.&lt;/p&gt;
&lt;p&gt;Following the publication of this PsyPost article somebody 
&lt;a href=&#34;https://old.reddit.com/r/science/comments/jyfvw2/study_suggests_the_covid19_pandemic_has_altered/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;posted the paper to Reddit&lt;/a&gt;, where it got considerable attention with nearly 40,000 upvotes and a great discussion of over 1,800 comments. Many of the comments described personal anecdotes regarding how people have experienced the same effect we report in the paper, that experiencing a global pandemic makes us recognize that poverty is not just manifest laziness. For example, one redditor said:&lt;/p&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;The same thing happened with FDR because of polio. In his memoir he recounted laying in the polio ward and the many lying in the bed next to him was an old, poor, black man. Now he was a young, rich, white boy from OLD New England money. But there he was, laying there dying next to this man who was worlds apart from him, but in the end it didn&amp;rsquo;t matter. The disease came for everyone regardless.&lt;br&gt;&lt;br&gt;And as he spend weeks in the ward he began to contemplate how it must be similar with wealth. There were people out there who didn&amp;rsquo;t catch polio and from where he sat the only thing different for them was luck and circumstance. And for all those who did catch it, it wasn&amp;rsquo;t because they&amp;rsquo;d done something wrong to deserve it, he certainly didn&amp;rsquo;t. So maybe, just maybe, it was the same with wealth. Wealth was more a matter of luck and circumstance than any intrinsic trait of the posser. And so his plans for expanding social programs for Americans began to take shape.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;I found it insightful to read about how the findings of our paper rang true for many people experiencing difficulty amidst the pandemic. The thread is worth a read!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Coronavirus crisis may help Americans remember that economic inequality is not fair or just</title>
      <link>/project/latimes_covid/</link>
      <pubDate>Wed, 22 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/project/latimes_covid/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Why are the Poor Poor and Why Does it Matter?</title>
      <link>/project/nhb_behindthepaper/</link>
      <pubDate>Mon, 16 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/project/nhb_behindthepaper/</guid>
      <description></description>
    </item>
    
    <item>
      <title>TBT Our SPSP 2015 Symposium on Economic Mobility and Inequality</title>
      <link>/project/spsp_2015/</link>
      <pubDate>Wed, 17 Oct 2018 00:00:00 -0500</pubDate>
      <guid>/project/spsp_2015/</guid>
      <description>&lt;p&gt;Back in 2015, when I was a second year Master&amp;rsquo;s student I co-chaired a symposium at the annual Society for Personality and Social Psychology (SPSP) conference with 
&lt;a href=&#34;https://www.shaidavidai.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Shai Davidai&lt;/a&gt;. In short, across three great talks given by Drs. Mike Norton and Shai Davidai, as well as myself, we covered:&lt;/p&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;High economic inequality is often justified by the belief in social mobility, the possibility that
anyone can increase their economic standing through hard work. Three speakers discuss new
research on subjective perceptions of inequality and social mobility, and the how these
perceptions impact emotional well-being.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;We were lucky enough to have videographers at SPSP film each of the talks and post them on youtube. So, I thought I would archive them here with their abstracts. Take a trip back to SPSP of (almost) four years ago (!!) and see some of the cutting edge work on economic inequality and mobility.&lt;/p&gt;
&lt;h1 id=&#34;dr-mike-norton&#34;&gt;Dr. Mike Norton&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.hbs.edu/faculty/Pages/profile.aspx?facId=326229&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mike&amp;rsquo;s&lt;/a&gt; talk was titled &amp;ldquo;How Much (More) Should CEOs Make? A Universal Desire for More Equal Pay&amp;rdquo; and was based on 
&lt;a href=&#34;http://journals.sagepub.com/doi/abs/10.1177/1745691614549773&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this&lt;/a&gt; paper. Here is the abstract for the talk:&lt;/p&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;We assess people&#39;s preferred wage differentials between rich and poor, and determine whether
these ideal ratios are commonly-held. Using survey data from 40 countries (N = 55,238), we
compare respondents&#39; estimates of the actual wages of chief executive officers and unskilled
workers to their ideals for what those wages should be. We show that ideal pay gaps between
CEOs and unskilled workers are significantly smaller than estimated pay gaps, and that there is
consensus across countries, socioeconomic status, and political beliefs for ideal pay ratios.
Moreover, data from 16 countries reveals that people dramatically underestimate actual  pay
inequality. In the United States the actual pay ratio of CEOs to unskilled workers (354:1) far
exceeded the estimated ratio (30:1) which in turn far exceeded the ideal ratio (7:1). People
underestimate pay gaps, and their ideal pay gaps are even further from reality than their
erroneous underestimates.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/lHHetZogA34&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h1 id=&#34;dr-shai-davidai&#34;&gt;Dr. Shai Davidai&lt;/h1&gt;
&lt;p&gt;Shai&amp;rsquo;s talk was titled &amp;ldquo;Building a More Mobile America – One Income Quintile at a Time&amp;rdquo; and was based on 
&lt;a href=&#34;http://journals.sagepub.com/doi/abs/10.1177/1745691614562005&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this&lt;/a&gt; paper. Here is the abstract for the talk:&lt;/p&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;A core tenet of the American ethos is that there is considerable economic mobility. Americans
seem willing to accept vast financial inequalities as long as they believe that everyone has the
opportunity to succeed. We examined whether people’s beliefs about the amount of economic
mobility in the United States conform to reality. In a nationally representative sample
(N=3,034), we found that: (1) people believe there is more upward mobility than downward
mobility, (2) people overestimate the amount of upward mobility and underestimate the amount
of downward mobility and (3) poorer individuals believe there is more mobility than richer ones.
An additional study (N=290) replicated these results and found that political affiliation
influences perceptions of mobility, with conservatives believing that the economic system is
more dynamic than liberals do. We discuss these findings in terms of system justification theory
and consider the implications for contemporary political debates in the United States.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/iZ8bgcOeCPQ&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h1 id=&#34;myself&#34;&gt;Myself&lt;/h1&gt;
&lt;p&gt;My talk was titled &amp;ldquo;Belief in high social mobility and emotional well-being&amp;rdquo; and was based on 
&lt;a href=&#34;http://summit.sfu.ca/item/15479&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the work that became my Master&amp;rsquo;s Thesis&lt;/a&gt;. Here is the abstract for the talk:&lt;/p&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;The American Dream posits that anyone can move between income levels, but recent reports
document that income mobility is at an all-time low (Chetty, Hendren, Kline, &amp;amp; Saez, 2013).
High levels of income mobility may offer economic advantages, but does perceived mobility
impact well-being? Past research provides conflicting hypotheses, suggesting both positive and
negative well-being outcomes (Diener, Lucas, &amp;amp; Oishi, 2002; Smith, Loewenstein, Jankovic, &amp;amp;
Ubel, 2009). In Study 1(n=100) participants who believed they had higher income mobility
reported higher positive affect and life satisfaction. In Study 2 (n=456) participants randomly
assigned to read about high (vs. low) income mobility reported higher positive, and lower
negative, affect. In Study 3 (n=435) we replicated Study 2 in a nationally representative sample.
Across all three studies emotional benefits persisted regardless of the participants’ income level.
These findings suggest there are emotional benefits to perceiving high income mobility,
regardless of current economic standing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/lf_jReoh9No&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&#34;the-end&#34;&gt;The End!&lt;/h2&gt;
&lt;p&gt;And that&amp;rsquo;s it for our 2015 symposium! I hope you enjoyed! Please feel free to reach out if you have any questions!&lt;/p&gt;
&lt;p&gt;Bonus: I chaired another symposium on inequality at SPSP 2017. An undergraduate student writer wrote a post about Shai&amp;rsquo;s talk on &amp;ldquo;The Great Gatsby Curve&amp;rdquo; 
&lt;a href=&#34;http://www.spsp.org/news-center/blog/overestimate-economic-mobility&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Maching Learning Tutorial: Predicting Car Prices with K-Nearest Neighbours (KNN) Regression</title>
      <link>/project/car_prices/</link>
      <pubDate>Sun, 07 Oct 2018 00:00:00 -0500</pubDate>
      <guid>/project/car_prices/</guid>
      <description>&lt;h1 id=&#34;predicting-car-prices-with-knn-regression&#34;&gt;Predicting Car Prices with KNN Regression&lt;/h1&gt;
&lt;p&gt;In this brief tutorial I am going to run through how to build, implement, and cross-validate a simple k-nearest neighbours (KNN) regression model. Simply put, KNN model is a variant of simple linear regression wherein we utilize information about neighbouring data points to predict an unknown outcome. To put a more concrete spin on this, and use the name quite literally, we can think about this in the context of your next door neighbours. If the outcome I wanted to predict was your personal income and didn&amp;rsquo;t have any other information about you, besides where you live, I might be wise to just ask your next door neighbours. I might decide to run a real life 4-nearest neighbours test. The two houses to the right of yours have a household incomes of $65,000 and $90,000 respectively, and the two to the left $100,000 and $72,000. In the simplest possible fashion I would then just assume you make somewhere in the ballpark of what your neighbours make, take an average, and predict that you have a household income of $81,750.&lt;/p&gt;
&lt;p&gt;This is an extremely basic explanation of how KNN algorithms work. But how do we decide who someone&amp;rsquo;s &amp;ldquo;neighbours&amp;rdquo; are when we aren&amp;rsquo;t talking about literal neighbours? That is, how do we know what other observations are similar to our target? Enter: distance metrics. The most common distance metric in for knn regression problems is 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Euclidean_distance&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Euclidian distance&lt;/a&gt;. This isn&amp;rsquo;t meant to be an extremely math heavy tutorial, so suffice it to say that in running a knn model, for each and every observation in a data set where we want to predict an outcome we grab their k-nearest neighbours, as defined by a metric such as Euclidean distance, from the training set, look at their values for the main dependent variable, and predict the new value based on the neighbours (e.g., as an average in a regression problem, or a &amp;ldquo;majority vote&amp;rdquo; in a classification problem).&lt;/p&gt;
&lt;p&gt;KNN regression can be used for both classification (i.e., predicting a binary outcome) and regression (i.e., predicting a continuous outcome). The procedure for implementation is largely the same and in this post I&amp;rsquo;m going to focus on regression. Specifically, the question at hand is: &lt;strong&gt;can we predict how much a used car is going to sell for?&lt;/strong&gt; For this question I am going to utilize a data set from the 
&lt;a href=&#34;https://archive.ics.uci.edu/ml/datasets/automobile&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;machine learning repository at The University of California, Irvine&lt;/a&gt;. First up is to just take a brief look at what is actually in the dataset.&lt;/p&gt;
&lt;h2 id=&#34;brief-exploratory-analysis-and-cleaning&#34;&gt;Brief Exploratory Analysis and Cleaning&lt;/h2&gt;
&lt;p&gt;These data contain a ton of information on a lot of different cars. For the purposes of this tutorial, I&amp;rsquo;m just going to pull out a set of relevant columns and work with those.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd

# Need to specify the headers for this dataset
cols = [&amp;quot;symboling&amp;quot;, &amp;quot;normalized_losses&amp;quot;, &amp;quot;make&amp;quot;, &amp;quot;fuel_type&amp;quot;, &amp;quot;aspiration&amp;quot;,
       &amp;quot;num_doors&amp;quot;, &amp;quot;body_style&amp;quot;, &amp;quot;drive_wheels&amp;quot;, &amp;quot;engine_location&amp;quot;,
       &amp;quot;wheel_base&amp;quot;, &amp;quot;length&amp;quot;, &amp;quot;width&amp;quot;, &amp;quot;height&amp;quot;, &amp;quot;curb_weight&amp;quot;, &amp;quot;engine_type&amp;quot;,
       &amp;quot;num_cylinders&amp;quot;, &amp;quot;engine_size&amp;quot;, &amp;quot;fuel_system&amp;quot;, &amp;quot;bore&amp;quot;, &amp;quot;stroke&amp;quot;,
       &amp;quot;compression_ratio&amp;quot;, &amp;quot;horsepower&amp;quot;, &amp;quot;peak_rpm&amp;quot;, &amp;quot;city_mpg&amp;quot;, &amp;quot;highway_mpg&amp;quot;,
       &amp;quot;price&amp;quot;]
cars = pd.read_csv(&amp;quot;imports-85.data&amp;quot;, names=cols)
cars.dtypes

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;symboling              int64
normalized_losses     object
make                  object
fuel_type             object
aspiration            object
num_doors             object
body_style            object
drive_wheels          object
engine_location       object
wheel_base           float64
length               float64
width                float64
height               float64
curb_weight            int64
engine_type           object
num_cylinders         object
engine_size            int64
fuel_system           object
bore                  object
stroke                object
compression_ratio    float64
horsepower            object
peak_rpm              object
city_mpg               int64
highway_mpg            int64
price                 object
dtype: object
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, as you can see we&amp;rsquo;ve got 25 columns that might be informative in predicting a car&amp;rsquo;s sale price, ranging from both highway and city miles per gallon to the number of doors a car has. Let&amp;rsquo;s take a quick look at the first few rows of the data just so we can get a sense of how it actually looks.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cars.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;symboling&lt;/th&gt;
      &lt;th&gt;normalized_losses&lt;/th&gt;
      &lt;th&gt;make&lt;/th&gt;
      &lt;th&gt;fuel_type&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;horsepower&lt;/th&gt;
      &lt;th&gt;peak_rpm&lt;/th&gt;
      &lt;th&gt;city_mpg&lt;/th&gt;
      &lt;th&gt;highway_mpg&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;?&lt;/td&gt;
      &lt;td&gt;alfa-romero&lt;/td&gt;
      &lt;td&gt;gas&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;111&lt;/td&gt;
      &lt;td&gt;5000&lt;/td&gt;
      &lt;td&gt;21&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;13495&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;?&lt;/td&gt;
      &lt;td&gt;alfa-romero&lt;/td&gt;
      &lt;td&gt;gas&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;111&lt;/td&gt;
      &lt;td&gt;5000&lt;/td&gt;
      &lt;td&gt;21&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;16500&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;?&lt;/td&gt;
      &lt;td&gt;alfa-romero&lt;/td&gt;
      &lt;td&gt;gas&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;154&lt;/td&gt;
      &lt;td&gt;5000&lt;/td&gt;
      &lt;td&gt;19&lt;/td&gt;
      &lt;td&gt;26&lt;/td&gt;
      &lt;td&gt;16500&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;164&lt;/td&gt;
      &lt;td&gt;audi&lt;/td&gt;
      &lt;td&gt;gas&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;102&lt;/td&gt;
      &lt;td&gt;5500&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;13950&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;164&lt;/td&gt;
      &lt;td&gt;audi&lt;/td&gt;
      &lt;td&gt;gas&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;115&lt;/td&gt;
      &lt;td&gt;5500&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;17450&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 26 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;To keep things simple and focus just on numeric columns without much feature engineering for now, it seems like we can use wheelbase, length, width, height, engine size, compression ratio, and city/highway mpr to predict price. Some of these predictors probably offer more information that others (miles per gallon is probably more informative of a car&amp;rsquo;s sale price than curb weight).&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;re probably going to want to use some of the other variables that aren&amp;rsquo;t numeric - they are likely also meaningful. So, right now we&amp;rsquo;ll deal with counting and seeing where our missing values are, as well as turning relevant columns numeric so we can actually use them. You will have noticed above that there were some questionmarks in the data - we just need to turn those into missing values. So I do this in the code below, then I select sex non-numeric columns that might be meaningful (normalized_losses, bore, stroke, horsepower, peak_rpm, and price) and turn them numeric. These ones are easy as they are actually numbers, they just are currently stored as objects.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np

cars = cars.replace(&#39;?&#39;, np.nan)

# Now lets make things numeric
num_vars = [&#39;normalized_losses&#39;, &amp;quot;bore&amp;quot;, &amp;quot;stroke&amp;quot;, &amp;quot;horsepower&amp;quot;, &amp;quot;peak_rpm&amp;quot;,
            &amp;quot;price&amp;quot;]

for i in num_vars:
    cars[i] = cars[i].astype(&#39;float64&#39;)
    
cars.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;symboling&lt;/th&gt;
      &lt;th&gt;normalized_losses&lt;/th&gt;
      &lt;th&gt;make&lt;/th&gt;
      &lt;th&gt;fuel_type&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;horsepower&lt;/th&gt;
      &lt;th&gt;peak_rpm&lt;/th&gt;
      &lt;th&gt;city_mpg&lt;/th&gt;
      &lt;th&gt;highway_mpg&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;alfa-romero&lt;/td&gt;
      &lt;td&gt;gas&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;111.0&lt;/td&gt;
      &lt;td&gt;5000.0&lt;/td&gt;
      &lt;td&gt;21&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;13495.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;alfa-romero&lt;/td&gt;
      &lt;td&gt;gas&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;111.0&lt;/td&gt;
      &lt;td&gt;5000.0&lt;/td&gt;
      &lt;td&gt;21&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;16500.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;alfa-romero&lt;/td&gt;
      &lt;td&gt;gas&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;154.0&lt;/td&gt;
      &lt;td&gt;5000.0&lt;/td&gt;
      &lt;td&gt;19&lt;/td&gt;
      &lt;td&gt;26&lt;/td&gt;
      &lt;td&gt;16500.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;164.0&lt;/td&gt;
      &lt;td&gt;audi&lt;/td&gt;
      &lt;td&gt;gas&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;102.0&lt;/td&gt;
      &lt;td&gt;5500.0&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;13950.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;164.0&lt;/td&gt;
      &lt;td&gt;audi&lt;/td&gt;
      &lt;td&gt;gas&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;115.0&lt;/td&gt;
      &lt;td&gt;5500.0&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;17450.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 26 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Everything looks good now - how many missing values do we have in the normalized losses column?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;normalized losses: &amp;quot;, cars[&#39;normalized_losses&#39;].isnull().sum())
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;normalized losses:  41
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are 41 missing values in the normalized_losses column. Given there are only 205 rows, thats a decent chunk missing. I&amp;rsquo;m not sure this column is the most useful, so we&amp;rsquo;ll just not use this column in our analyses at all. Let&amp;rsquo;s take a look at our other numeric columns and see what the missing values are like. The below chunk just calculates the sum of missing values for each variable and displays that sum.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cars.isnull().sum()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;symboling             0
normalized_losses    41
make                  0
fuel_type             0
aspiration            0
num_doors             2
body_style            0
drive_wheels          0
engine_location       0
wheel_base            0
length                0
width                 0
height                0
curb_weight           0
engine_type           0
num_cylinders         0
engine_size           0
fuel_system           0
bore                  4
stroke                4
compression_ratio     0
horsepower            2
peak_rpm              2
city_mpg              0
highway_mpg           0
price                 4
dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So it looks like most of our columns are pretty good, with only a couple missing values here and there. The most crucial one here is price, our dependent variable; there are four cars that don&amp;rsquo;t have prices. Given that the number of missing rows is, at most, about 2%, I&amp;rsquo;m just going to listwise delete any row that has a missing variable in any of these. I don&amp;rsquo;t like mean imputation as it is purely making up data.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll start with the price column because its the most important and I suspect the rows that are missing price are the same rows missing the other data as well. Here I just drop any rows that are missing price data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cars = cars.dropna(subset = [&#39;price&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now lets check the missing values again, just to be sure it worked correctly:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cars.isnull().sum()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;symboling             0
normalized_losses    37
make                  0
fuel_type             0
aspiration            0
num_doors             2
body_style            0
drive_wheels          0
engine_location       0
wheel_base            0
length                0
width                 0
height                0
curb_weight           0
engine_type           0
num_cylinders         0
engine_size           0
fuel_system           0
bore                  4
stroke                4
compression_ratio     0
horsepower            2
peak_rpm              2
city_mpg              0
highway_mpg           0
price                 0
dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, I&amp;rsquo;ll do the same to listwise delete the other numeric columns.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cars = cars.dropna(subset = [&#39;bore&#39;, &#39;stroke&#39;, &#39;horsepower&#39;, &#39;peak_rpm&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we should have no missing data and be ready to go! The next step is to convert all the numeric columns into standardized z-scores. This is especially important if your variables are on drastically different scales. For instance here, horsepower is generally way up over 100 and miles per gallon is never more than about 45. So what I&amp;rsquo;ll do below is trim the dataset down just to the numeric columns, and then convert each of those columns into a z-score. Then, I save this into a new dataset called &amp;ldquo;normalized.&amp;rdquo; This is generally good practice because that way we retain our original dataset in case we need to go back to it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cols = [&#39;wheel_base&#39;, &#39;length&#39;, &#39;width&#39;, &#39;height&#39;,
        &#39;curb_weight&#39;, &#39;engine_size&#39;, &#39;bore&#39;, &#39;stroke&#39;, &#39;horsepower&#39;,
        &#39;peak_rpm&#39;, &#39;city_mpg&#39;, &#39;highway_mpg&#39;, &#39;price&#39;]
cars = cars[cols]

normalized_cars = (cars - cars.mean()) / (cars.std())
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;modeling&#34;&gt;Modeling&lt;/h2&gt;
&lt;p&gt;Alright, onward into some modeling! We&amp;rsquo;ve got a nice clean data set full of numeric columns. The first thing I&amp;rsquo;m going to do is create a couple of univariate (i.e., just one predictor) models, just to see how informative certain predictors are. Now, in more traditionally academic regression contexts this would be akin to just running some linear regressions with individual predictors. For example, we might see how well highway miles per gallon &amp;ldquo;predicts&amp;rdquo; sale price on it&amp;rsquo;s own. Of course, in the academic context, when we say predict what we actually mean is &amp;ldquo;variance explained&amp;rdquo; - we&amp;rsquo;re really finding out how much of the variance in sale price can be explained just by looking at highway miles per gallon.&lt;/p&gt;
&lt;p&gt;In the machine learning context, we are actually more concerned with &lt;em&gt;prediction&lt;/em&gt;. That is, if we build a KNN model, where we were only identifying neighbours based on how similar they were in highway miles per gallon, could we accurately predict price? There are myriad different ways we could judge accuracy, but here I&amp;rsquo;m going to use 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Root-mean-square_deviation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Root Mean Squared Error (RMSE)&lt;/a&gt;. RMSE is one of the most common error metrics for regression based machine learning. Again, this is not meant to be a math-heavy tutorial so I won&amp;rsquo;t go into it deeply here but it quantifies how far off our predictions were from the actual values.&lt;/p&gt;
&lt;p&gt;So, to start running some very basic univariate KNN models I imported two pieces of the sklearn package below, one for training a KNN model (KNeighborsRegressor) and one for calculating the mean squared error (mean_squared_error), from which we will derive the RMSE. Now, we need to do a couple things to build these models. Specifically, we need to choose the predictor we want to test, choose the dependent variable, and split our data into training and test sets so we reduce the risk of 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Overfitting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;overfitting&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To accomplish this, I define a function that takes in three arguments: (1) our training column(s), (2), our target column, and (3) the dataset to use. Using this information, the function first instatiates an instance of a k-nearest neighbours regression (stored as &amp;ldquo;knn&amp;rdquo;) and sets a set so our results are reproducible. Next, the function shuffles the data into a random order, splits the data in half, designates the top half as the training data, and the bottom half as the test data.&lt;/p&gt;
&lt;p&gt;Then, we get down to the nitty gritty. The function fits the knn object on the specified training and test columns of the training data, uses that model to make predictions on the test data, and then calculates the RMSE (e.g., the difference between the predictions our model made for each car in the test set&amp;rsquo;s price and the actual prices).&lt;/p&gt;
&lt;p&gt;As we move through, we&amp;rsquo;ll complicate this function bit by bit, adding extra stuff to it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Writing a simple function that trains and tests univariate models
# This function takes in three arguments: the predictor, the outcome, &amp;amp; the data
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error

def knn_train_test(train_col, target_col, df):
    knn = KNeighborsRegressor()
    np.random.seed(1)
        
    # Randomize order of rows in data frame.
    shuffled_index = np.random.permutation(df.index)
    rand_df = df.reindex(shuffled_index)

    # Divide number of rows in half and round.
    last_train_row = int(len(rand_df) / 2)
    
    # Select the first half and set as training set.
    # Select the second half and set as test set.
    train_df = rand_df.iloc[0:last_train_row]
    test_df = rand_df.iloc[last_train_row:]
    
    # Fit a KNN model using default k value.
    knn.fit(train_df[[train_col]], train_df[target_col])
    
    # Make predictions using model.
    predicted_labels = knn.predict(test_df[[train_col]])

    # Calculate and return RMSE.
    mse = mean_squared_error(test_df[target_col], predicted_labels)
    rmse = np.sqrt(mse)
    return rmse
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we&amp;rsquo;ve got this function defined, let&amp;rsquo;s use it! If you recall, I said I was going to just test some basic univariate models. So, I&amp;rsquo;m going to run our new function five times, getting the RMSE of five different predictors. I just chose four that I thought would be relatively meaningful in predicting price (city and highway miles per gallon, engine size, and horsepower) and one to serve as a logic check (width) - why would width predict the price of a car, unless larger vehicles are more expensive. In any case, my intuition suggests that width should be the word predictor of price.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Lets test a couple of predictors
print(&#39;city mpg: &#39;, knn_train_test(&#39;city_mpg&#39;, &#39;price&#39;, normalized_cars))
print(&#39;width: &#39;, knn_train_test(&#39;width&#39;, &#39;price&#39;, normalized_cars))
print(&#39;highway mpg: &#39;, knn_train_test(&#39;highway_mpg&#39;, &#39;price&#39;, normalized_cars))
print(&#39;engine size: &#39;, knn_train_test(&#39;engine_size&#39;, &#39;price&#39;, normalized_cars))
print(&#39;horsepower: &#39;, knn_train_test(&#39;horsepower&#39;, &#39;price&#39;, normalized_cars))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;city mpg:  0.598975486019
width:  0.671608148246
highway mpg:  0.537913994132
engine size:  0.536691465842
horsepower:  0.571585852136
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As I suspected, width is by quite a large margin the worst predictor of a car&amp;rsquo;s price. Of the couple predictors that I threw in there to test, the best most informative for determining a vehicles price seems to be it&amp;rsquo;s highway. So, if we wanted to be as accurate as possible while only using on predictor, we would want to use fuel economy on the highway.&lt;/p&gt;
&lt;h2 id=&#34;hyperparamaterization&#34;&gt;&amp;ldquo;Hyperparamaterization&amp;rdquo;&lt;/h2&gt;
&lt;p&gt;If you recall, in KNN regression, we can set k to be whatever we want: 3, 5, 7, 100, 1000. Common values of k range from 3 to 10 - does tweaking our k value, and grabbing more or less neighbours to make our price guess, make the model fit better? As a test of this question, I&amp;rsquo;m going to modify the above function to take another argument: a k value. Then, I&amp;rsquo;ll test each of the five predictors above (plus some more) each with five different values of k (1, 3, 5, 7, and 9). This will effectively run our regression 25 times; city mpg with 1 neighbour, city mpg with 2 neighbours, and so on.&lt;/p&gt;
&lt;p&gt;The way I&amp;rsquo;ve done this is to insert a list of k-values into the middle of the function, and set up an empty dictionary to store all our RMSEs. Then, I nested the code from previously that fits the model, generates our predictions, and calculates the RSME into a for loop that does this for each value of k. Lastly, it appends the RMSE to the dictionary and returns it.&lt;/p&gt;
&lt;p&gt;Lastly, I specified a list of all the columns I want to build univariate models for, use a for loop to run the function on each of those columns, and append the results to another dictionary called &amp;ldquo;k_rmse_results.&amp;rdquo; Printing this dictionary gives us the name of the predictor, the specified k, and then the RMSE.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def knn_train_test_new(train_col, target_col, df):
    np.random.seed(1)
        
    # Randomize order of rows in data frame.
    shuffled_index = np.random.permutation(df.index)
    rand_df = df.reindex(shuffled_index)

    # Divide number of rows in half and round.
    last_train_row = int(len(rand_df) / 2)
    
    # Select the first half and set as training set.
    # Select the second half and set as test set.
    train_df = rand_df.iloc[0:last_train_row]
    test_df = rand_df.iloc[last_train_row:]
    
    k_values = [1,3,5,7,9]
    k_rmses = {}
    
    for k in k_values:
        # Fit model using k nearest neighbors.
        knn = KNeighborsRegressor(n_neighbors=k)
        knn.fit(train_df[[train_col]], train_df[target_col])

        # Make predictions using model.
        predicted_labels = knn.predict(test_df[[train_col]])

        # Calculate and return RMSE.
        mse = mean_squared_error(test_df[target_col], predicted_labels)
        rmse = np.sqrt(mse)
        
        k_rmses[k] = rmse
    return k_rmses

k_rmse_results = {}

# For each column from above, train a model, return RMSE value
# and add to the dictionary `rmse_results`.
variables = [&#39;wheel_base&#39;, &#39;length&#39;, &#39;width&#39;, &#39;height&#39;,
        &#39;curb_weight&#39;, &#39;engine_size&#39;, &#39;bore&#39;, &#39;stroke&#39;, &#39;horsepower&#39;,
        &#39;peak_rpm&#39;, &#39;city_mpg&#39;, &#39;highway_mpg&#39;]

for var in variables:
    rmse_val = knn_train_test_new(var, &#39;price&#39;, normalized_cars)
    k_rmse_results[var] = rmse_val

k_rmse_results
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&#39;bore&#39;: {1: 1.2142304178718561,
  3: 0.86766581048215152,
  5: 0.89458788943880752,
  7: 0.94676716177240661,
  9: 0.95385344053196963},
 &#39;city_mpg&#39;: {1: 0.69529747854104784,
  3: 0.59031417913396289,
  5: 0.59897548601904338,
  7: 0.59715938629269016,
  9: 0.57728649652220132},
 &#39;curb_weight&#39;: {1: 0.8365387787670262,
  3: 0.64395375801733934,
  5: 0.57031290606074236,
  7: 0.51644149986604171,
  9: 0.51839468763038343},
 &#39;engine_size&#39;: {1: 0.55456842477058543,
  3: 0.54125650939355474,
  5: 0.53669146584152094,
  7: 0.51899873944760311,
  9: 0.50362821678292591},
 &#39;height&#39;: {1: 1.1521894508998922,
  3: 1.0168354498989998,
  5: 0.94018342361170537,
  7: 0.98192402693779424,
  9: 0.94562106614391528},
 &#39;highway_mpg&#39;: {1: 0.69402405950428248,
  3: 0.57416390399142236,
  5: 0.53791399413219376,
  7: 0.53495996840130122,
  9: 0.54899088943686292},
 &#39;horsepower&#39;: {1: 0.54931712010379319,
  3: 0.58337889657418729,
  5: 0.57158585213578872,
  7: 0.59314672824644243,
  9: 0.59002690698940097},
 &#39;length&#39;: {1: 0.65713817747103875,
  3: 0.63950652453727652,
  5: 0.64700844560860649,
  7: 0.66892394999830362,
  9: 0.6572441270128111},
 &#39;peak_rpm&#39;: {1: 0.85899385207057188,
  3: 0.88634665981443039,
  5: 0.90984280257609562,
  7: 0.90757986581712302,
  9: 0.90022237747155309},
 &#39;stroke&#39;: {1: 0.91796375714948086,
  3: 0.88113492952413897,
  5: 0.89586641314603555,
  7: 0.94574853212680499,
  9: 0.92121730389055356},
 &#39;wheel_base&#39;: {1: 0.70984063675235898,
  3: 0.70981264790591259,
  5: 0.70662378566203043,
  7: 0.71073380297018984,
  9: 0.72499494497288963},
 &#39;width&#39;: {1: 0.77543242861316763,
  3: 0.68812005931548847,
  5: 0.67160814824580428,
  7: 0.5826291501627695,
  9: 0.56790774193128279}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Generally speaking, just eyeballing within each predictor, going up to k = 9 worked the best for some of the models, but not for others. The error actually went up as we increased k for some. With a k of 9, the best univariate predictor seems to be engine size, with predictions only off by about half a standard deviation. So thats the best so far, but it&amp;rsquo;s still not great. When trying to predict an outcome, univariate models aren&amp;rsquo;t going to be extremely informative. Outcomes are complex, so let&amp;rsquo;s build a model that reflects that and takes in more than one column.&lt;/p&gt;
&lt;p&gt;What I&amp;rsquo;ve done to accomplish this is modified the above function to take multiple columns and then train and test some models using the the best two, then three, four, and five columns from above. By eyeballing at the 5-nearest neighbours level, the columns that predict the best in isolation are engine size, highway mpg, curb_weight, horsepower, and city mpg.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def knn_train_test_mult(train_cols, target_col, df):
    np.random.seed(1)
        
    # Randomize order of rows in data frame.
    shuffled_index = np.random.permutation(df.index)
    rand_df = df.reindex(shuffled_index)

    # Divide number of rows in half and round.
    last_train_row = int(len(rand_df) / 2)
    
    # Select the first half and set as training set.
    # Select the second half and set as test set.
    train_df = rand_df.iloc[0:last_train_row]
    test_df = rand_df.iloc[last_train_row:]
    
    k_values = [5]
    k_rmses = {}
    
    for k in k_values:
        # Fit model using k nearest neighbors.
        knn = KNeighborsRegressor(n_neighbors=k)
        knn.fit(train_df[train_cols], train_df[target_col])

        # Make predictions using model.
        predicted_labels = knn.predict(test_df[train_cols])

        # Calculate and return RMSE.
        mse = mean_squared_error(test_df[target_col], predicted_labels)
        rmse = np.sqrt(mse)
        
        k_rmses[k] = rmse
    return k_rmses

train_cols_2 = [&#39;engine_size&#39;, &#39;highway_mpg&#39;]
train_cols_3 = [&#39;engine_size&#39;, &#39;highway_mpg&#39;, &#39;curb_weight&#39;]
train_cols_4 = [&#39;engine_size&#39;, &#39;highway_mpg&#39;, &#39;curb_weight&#39;,
               &#39;horsepower&#39;]
train_cols_5 = [&#39;engine_size&#39;, &#39;highway_mpg&#39;, &#39;curb_weight&#39;,
               &#39;horsepower&#39;, &#39;city_mpg&#39;]

k_rmse_results = {}

rmse_val = knn_train_test_mult(train_cols_2, &#39;price&#39;, normalized_cars)
k_rmse_results[&amp;quot;two best features&amp;quot;] = rmse_val
rmse_val = knn_train_test_mult(train_cols_3, &#39;price&#39;, normalized_cars)
k_rmse_results[&amp;quot;three best features&amp;quot;] = rmse_val
rmse_val = knn_train_test_mult(train_cols_4, &#39;price&#39;, normalized_cars)
k_rmse_results[&amp;quot;four best features&amp;quot;] = rmse_val
rmse_val = knn_train_test_mult(train_cols_5, &#39;price&#39;, normalized_cars)
k_rmse_results[&amp;quot;five best features&amp;quot;] = rmse_val

k_rmse_results
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&#39;five best features&#39;: {5: 0.49634970383078636},
 &#39;four best features&#39;: {5: 0.48157162981314988},
 &#39;three best features&#39;: {5: 0.50707231602531166},
 &#39;two best features&#39;: {5: 0.43695579560820619}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, what we have here is four new models that combine the best features we identified above. For example, the two best features model is using engine size and highway mpg to predict sale price with k set to 5. The three best predictors model is using engine size, highway mpg, and curb weight. Interestingly, adding more predictors does not increase the performance of the model at all - in fact, it makes it worse. This is not at all surprising because we started with the two best predictors, and just tacked on worse and worse predictors. So simply adding more doesn&amp;rsquo;t do anything good for us, it just adds muddier predictors to the model.&lt;/p&gt;
&lt;p&gt;The best performing model here to predict the price of a car is simply it&amp;rsquo;s engine size and highway miles per gallon. Let&amp;rsquo;s combine the two tests we&amp;rsquo;ve done so far and go a little overboard with it. Let&amp;rsquo;s test are four multivariate models at all ks ranging from 1 to 25.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def knn_train_test_mult(train_cols, target_col, df):
    np.random.seed(1)
        
    # Randomize order of rows in data frame.
    shuffled_index = np.random.permutation(df.index)
    rand_df = df.reindex(shuffled_index)

    # Divide number of rows in half and round.
    last_train_row = int(len(rand_df) / 2)
    
    # Select the first half and set as training set.
    # Select the second half and set as test set.
    train_df = rand_df.iloc[0:last_train_row]
    test_df = rand_df.iloc[last_train_row:]
    
    k_values = list(range(1,25))
    k_rmses = {}
    
    for k in k_values:
        # Fit model using k nearest neighbors.
        knn = KNeighborsRegressor(n_neighbors=k)
        knn.fit(train_df[train_cols], train_df[target_col])

        # Make predictions using model.
        predicted_labels = knn.predict(test_df[train_cols])

        # Calculate and return RMSE.
        mse = mean_squared_error(test_df[target_col], predicted_labels)
        rmse = np.sqrt(mse)
        
        k_rmses[k] = rmse
    return k_rmses

k_rmse_results_2 = {}

rmse_val = knn_train_test_mult(train_cols_2, &#39;price&#39;, normalized_cars)
k_rmse_results_2[&amp;quot;two best features&amp;quot;] = rmse_val
rmse_val = knn_train_test_mult(train_cols_3, &#39;price&#39;, normalized_cars)
k_rmse_results_2[&amp;quot;three best features&amp;quot;] = rmse_val
rmse_val = knn_train_test_mult(train_cols_4, &#39;price&#39;, normalized_cars)
k_rmse_results_2[&amp;quot;four best features&amp;quot;] = rmse_val
rmse_val = knn_train_test_mult(train_cols_5, &#39;price&#39;, normalized_cars)
k_rmse_results_2[&amp;quot;five best features&amp;quot;] = rmse_val

k_rmse_results_2

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&#39;five best features&#39;: {1: 0.48370851070283638,
  2: 0.45709593736092696,
  3: 0.45932846135166017,
  4: 0.47702516372024495,
  5: 0.49634970383078636,
  6: 0.51326367838750764,
  7: 0.49909158266517145,
  8: 0.49234720868686166,
  9: 0.49925986215124729,
  10: 0.50320132862625788,
  11: 0.49352187011331128,
  12: 0.50969596379759741,
  13: 0.51666729281793589,
  14: 0.52430330537451841,
  15: 0.52831175797337293,
  16: 0.53372729477563852,
  17: 0.53629514907736431,
  18: 0.53771760713738892,
  19: 0.54366780131360726,
  20: 0.55390626006954025,
  21: 0.56186625263562961,
  22: 0.56399228907515964,
  23: 0.56686293421110434,
  24: 0.56749133395228601},
 &#39;four best features&#39;: {1: 0.46011695087263338,
  2: 0.47254209382853879,
  3: 0.4957675847479055,
  4: 0.47457606714743178,
  5: 0.48157162981314988,
  6: 0.4988487989184765,
  7: 0.51321993316267744,
  8: 0.51485477394479984,
  9: 0.51767653423898286,
  10: 0.5323478616627898,
  11: 0.51621784291210482,
  12: 0.51180481420952084,
  13: 0.52093660357028182,
  14: 0.5264585716201885,
  15: 0.53375627848681184,
  16: 0.53867876043121632,
  17: 0.539751445854185,
  18: 0.54052905274747964,
  19: 0.54394381259000146,
  20: 0.55263828123323866,
  21: 0.55895871633176053,
  22: 0.56273074037859749,
  23: 0.55965116497813094,
  24: 0.56079209060485291},
 &#39;three best features&#39;: {1: 0.53304672541629627,
  2: 0.47236953984714419,
  3: 0.48544239785342919,
  4: 0.46468465687861726,
  5: 0.50707231602531166,
  6: 0.50672338800144279,
  7: 0.52042141176821866,
  8: 0.51959755832321974,
  9: 0.51531096459267978,
  10: 0.52080169867439063,
  11: 0.52322240088596494,
  12: 0.52075280982124794,
  13: 0.51183217030638006,
  14: 0.51868070682253542,
  15: 0.52774088364145721,
  16: 0.53035523273810814,
  17: 0.53183552745587126,
  18: 0.53451532836203997,
  19: 0.54155167597224763,
  20: 0.54551779898598018,
  21: 0.54674295619522062,
  22: 0.54707824469373634,
  23: 0.54985793303062935,
  24: 0.55512440029648535},
 &#39;two best features&#39;: {1: 0.4962211708123152,
  2: 0.42439666654800412,
  3: 0.37244955551446796,
  4: 0.38221587546513652,
  5: 0.43695579560820619,
  6: 0.49363489340281513,
  7: 0.50823941279743867,
  8: 0.51418965195989808,
  9: 0.52616341995960625,
  10: 0.53013621032483371,
  11: 0.53671709358748898,
  12: 0.53444184258976579,
  13: 0.51587142155192167,
  14: 0.51205713655316698,
  15: 0.5119686970820041,
  16: 0.51750488428455055,
  17: 0.52218955380387977,
  18: 0.53175784405886029,
  19: 0.54377416147308744,
  20: 0.54302569583633942,
  21: 0.54968364588915308,
  22: 0.5508362135961884,
  23: 0.55132842211110611,
  24: 0.55665088230669157}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can probably tell, it&amp;rsquo;s hard to make sense of this. For our different models with 2, 3, 4, and 5 predictors, what is the most accurate value of k? The best way to explore this might be just to plot it out! So, I&amp;rsquo;ll take the big dictionary that&amp;rsquo;s printed above and plot it out with RMSE on the y axis, k on the x axis, and then each colored line is a different model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt
% matplotlib inline

for k,v in k_rmse_results_2.items():
    x = list(v.keys())
    y = list(v.values())
    
    plt.plot(x,y)
    plt.xlabel(&#39;k value&#39;)
    plt.ylabel(&#39;RMSE&#39;)
    
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=figure&gt;&lt;img src=/project/Car_Prices/output_27_0.png&gt;&lt;/div&gt;
&lt;p&gt;The pattern you can discern just looking at the numbers becomes clear here when looking at the graph. The best models for any given set of predictors is right around a k of 3-5. Once we start adding more than that, the performance of the model gets worse.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;So, we&amp;rsquo;ve learned a lot about how to run a KNN regression using python, how to tweak paramaters like the number of predictors in a model and the number of neighbors we use, and how to evaluate those models.&lt;/p&gt;
&lt;p&gt;From this, we learned in our simple set of predictions that the most accurate model is using just the two best predictors, engine size and highway miles per gallon, with k set somewhere in the 3-5 range.&lt;/p&gt;
&lt;p&gt;I hope you found this tutorial helpful, and I encourage you to give it a shot for yourself using this dataset, or any other, from UCI&amp;rsquo;s rich machine learning repository!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Does High Economic Inequality, and Low Economic Mobility Threaten, the Relationship Between Income and Happiness?</title>
      <link>/project/mobility_happiness/</link>
      <pubDate>Fri, 05 Oct 2018 00:00:00 -0500</pubDate>
      <guid>/project/mobility_happiness/</guid>
      <description>&lt;p&gt;Myriad past research demonstrates a fairly strong link between money and happiness. That is, people are generally happier when they make more money up to around the $75,000 a year mark 
&lt;a href=&#34;http://www.pnas.org/content/pnas/107/38/16489.full.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(e.g., Kahneman &amp;amp; Deaton, 2010)&lt;/a&gt;. However, one possible threat to this relationship is the level of economic inequality. Really, one could imagine this relationship going in either directions. On one hand, it seems intuitive that money has more happiness purchasing-power when your immediate community is highly unequal. In this case, having a lot of money could make you more satisfied to have &amp;lsquo;made it.&amp;rsquo; On the other hand, it could be extremely uncomfortable to be on the top of the economic ladder when poverty is so readily apparent.&lt;/p&gt;
&lt;p&gt;I tested this question with a set of multilevel models using four different datasets, which can be found 
&lt;a href=&#34;https://github.com/dwiwad/Inequality-Mobility-Income-and-Happiness/tree/master/Data&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;survey_data.csv&lt;/strong&gt;; This dataset contains 1,441 survey responses from two qualtrics national panels. The key individual variables here are happiness, economic quintile, age, political ideology, and location (latitude and longitude). We collected these data in our lab as part of larger projects exploring the psychological correlates of perceived economic mobility.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ACS_14_5YR_B19083&lt;/strong&gt;; this dataset is from the United States census and contains two county identifiers (FIPS code and county name), income inequality (Gini) for each county, and the standard error for each Gini coefficient.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;gini.by.state&lt;/strong&gt;; this dataset is also from the United States census and contains two state identifiers (FIPS code and state name), income inequality (Gini) for each state, and the standard error for each Gini coefficient.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;mobility.by.county&lt;/strong&gt;; this dataset is from the Harvard Mobility Project and contains a measure of income mobility, as well as various population demographics, for each county. The measure I will be using, absolute upward mobility, quantifies the average income percentile for a child whose parents were in the 25th percentile. So, for example, if a county has an absolute upward mobility value of 40 this means that the children of parents who were in the 25th percentile of the income distribution ended up, on average, in the 40th percentile.&lt;/p&gt;
&lt;p&gt;So, some of the data was data we had collected in the past and some of the data came from established sources. The first thing I did was take a quick look at where our participants were in the United  States.&lt;/p&gt;
&lt;div class=figure&gt;&lt;img src=/project/Mobility_Happiness/geo_locate.png&gt;&lt;/div&gt;
&lt;p&gt;So, we can see that the participants are pretty spread out across the United States, with a bit of a dearth in the central United States; most participants seem to be in the Eastern U.S. I used this geolocation data to assign each participant the county in which they live. I will spare you the nitty-gritty details of the multilevel modeling, but if you are so inclined you can find everything 
&lt;a href=&#34;https://github.com/dwiwad/Inequality-Mobility-Income-and-Happiness/blob/master/Markdown%20Docs/MLM_Comprehensive_Exam.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;What I found was that, in line with past research, personal income (as measured by which income quintile the person reports being in) was a very strong predictor of happiness. In accounting for all the individual (age, gender, political ideology) and county-level (inequality) factors, it income was the only significant determinant of happiness. While I didn&amp;rsquo;t observe a significant interaction between income and happiness, suggesting that there could be a relationship here. But what does that relationship actually look like?&lt;/p&gt;
&lt;p&gt;In order to take a look at this, I did a quartile split on income inequality and binned participants into four categories: extremely low inequality (gini less than .43), low inequality (gini between .43 and .45), high inequality (gini more than .45 and less than.48), and very  high inequality (gini more than .48). Then, I plotted out the simple relationshp between income quintile and happiness in each of these inequality categories.&lt;/p&gt;
&lt;div class=figure&gt;&lt;img src=/project/Mobility_Happiness/ineq_hap_simp.png&gt;&lt;/div&gt;
&lt;p&gt;What you can see here pretty clearly is that the slope of the relationship in counties with less inequality (the green and orange lines) is steeper than in counties with more inequality (the purple and pink lines). That is to say, money seems to have more &amp;ldquo;happiness purchasing power&amp;rdquo; when people live in more equal communities. Just to look at this in another way, here are the data plotted as actual means instead of over simplified regression lines.&lt;/p&gt;
&lt;div class=figure&gt;&lt;img src=/project/Mobility_Happiness/ineq_hap.png&gt;&lt;/div&gt;
&lt;p&gt;The above presented data show that the increase we get in happiness from having more money is diminished in places with higher inequality. One possible reason for this is simply exposure to inequality through poverty. Looking only at the people who reported being in the fifth quintile, you can see that happiness is the lowest in the high and extremely high inequality places. It’s possible that in these places the high income people are exposed to more poverty, and thus feel an increased sense of wealth guilt, leading to a dampened general happiness.&lt;/p&gt;
&lt;p&gt;This is only one possible explanation and uncovering the mechanism here requires significant further exploration. For now, I’m going to just explore the data again, this time looking at the level of absolute upward mobility present in a county, instead of inequality.&lt;/p&gt;
&lt;h2 id=&#34;income-and-happiness-by-county-upward-mobility&#34;&gt;Income and Happiness by County Upward Mobility&lt;/h2&gt;
&lt;p&gt;One might suspect an interaction here such that the relationship between income and happiness is stronger in places with low mobility, perhaps as a sort of dissonance mechanism. That is, when one cannot move up the income ladder they rationalize and are thus happier with their level of income, regardless. Specifically, I would think that high income people are equally happy regardless of the level of mobility, but as mobility drops the baseline level of happiness for those in lower quintiles rises. Same with the previous analysis, I&amp;rsquo;m going to spare you all the nitty-gritty modeling details here and just present the main trends.&lt;/p&gt;
&lt;div class=figure&gt;&lt;img src=/project/Mobility_Happiness/mob_hap_simp.png&gt;&lt;/div&gt;
&lt;p&gt;This actually doesnt look all that different from the inequality version of the graph, but here there is no interaction whatsoever. The slopes are all equal. Let’s take a quick look at the graph of the actual data, instead of the regression lines:&lt;/p&gt;
&lt;div class=figure&gt;&lt;img src=/project/Mobility_Happiness/mob_hap.png&gt;&lt;/div&gt;
&lt;p&gt;Looking at this graph versus the graph with county inequality makes it clear there really is no interaction here, just as the MLM data show. These models suggest that the relationship between income and happiness may indeed be influenced by the level of inequality present in the county one lives. Specifically, income has greater happiness purchasing power when you live in a more equal society. Perhaps this is due to the decreased availability of visible poverty and inequality, leading to a lower sense of wealth guilt among those living in the higher quintiles. On the other hand, it is also possible that in areas with lower inequality do not suffer so much from the middle class being washed out, thus having a higher income means one’s purchasing power is higher and can live relatively better off compared with someone of the same income bracket in a high-inequality city, where their money potentiall has less purchasing power.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In sum, across this analysis I: (a) cleaned and combined four separate datasets into one useable dataset with individual and county level information, (b) ran a series of multilevel models exploring the interaction of county-level data and individual level relationships, and (C) unpacked these interactions with concise data visualization. Within the analyses, I first replicated a long-standing effect showing that higher wealth is related to higher happiness, overall. I then built upon this, showing that there appears to be a modest interaction between the level of inequality where one lives and the strength of the money-happiness relationship. Particularly, it appears that the happiness-purchasing power of money is greater when one lives in a more equal county. Lastly, I found that the level of absolute upward mobility in a county does not change the nature of the money-happiness relationship&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Visualizing the Tour De France</title>
      <link>/project/tdf_speed/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 -0500</pubDate>
      <guid>/project/tdf_speed/</guid>
      <description>&lt;p&gt;In this short post I&amp;rsquo;m going to do the first set of two visualizations exploring data regarding the grand tours that I pulled from Wikipedia. Specifically, here I&amp;rsquo;m going to look at a couple elements of the Tour de France over it&amp;rsquo;s entire 114 year history. Namely, I&amp;rsquo;m going to look at the length of the Tour, the average speed of the winner for each edition, their total winning time, and the margin that they won by.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m not intending to do any inferential analyses, but what I&amp;rsquo;m expecting to see is that the Tour is getting faster and shorter, and is being won on smaller margins as time goes on. I&amp;rsquo;m going to hide all of the code in this particular notebook, but I will display some of the raw data and explain the process as I go along just so you can see where I started and where I got to, and then what those data actually look like. If you are interested in the more technical aspects of how I did all of these analyses, there is a seperate jupyter notebook in the repository I put up on github for this particular project 
&lt;a href=&#34;https://github.com/dwiwad/Analyzing-the-Grand-Tours&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. These notebooks have more detailed information on what I did step by step, including all of the code.&lt;/p&gt;
&lt;p&gt;With that, let&amp;rsquo;s get right into the data!&lt;/p&gt;
&lt;h2 id=&#34;tour-de-france-winners-data&#34;&gt;Tour de France Winners Data&lt;/h2&gt;
&lt;p&gt;I grabbed these data from the wikipedia page for Tour GC winners, which can be found here on 
&lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_Tour_de_France_general_classification_winners&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the wikipedia page for Tour de France GC Winners.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s take a quick look at these raw data; what does it look like once scraped from wikipedia and cleaned a little bit? Here&amp;rsquo;s the first and last five rows (i.e., the first and most recent five Tour  winners):&lt;/p&gt;
&lt;div class=figure&gt;&lt;img src=/project/tdf_speed/tour_early.png&gt;&lt;/div&gt;
&lt;div class=figure&gt;&lt;img src=/project/tdf_speed/tour_late.png&gt;&lt;/div&gt;
&lt;p&gt;We&amp;rsquo;ve got a lot of interesting information in this table, including the winner, their team, the number of stage wins, etc. However, given that I&amp;rsquo;m focusing here on speed of the tour, I&amp;rsquo;m going to pull out all the information I need and put it into one smaller new table. That is, I&amp;rsquo;ll pull out the columns for distance, time, and margin.&lt;/p&gt;
&lt;p&gt;From this, I&amp;rsquo;ll calculate a new column, average speed, which is simply the total distance divided by the winning time. Here&amp;rsquo;s what the new cleaned up data look like:&lt;/p&gt;
&lt;div class=figure&gt;&lt;img src=/project/tdf_speed/tour_clean_head.png alt=&#34;&#34; width=&#34;65%&#34; height=&#34;65%&#34;/&gt;&lt;/div&gt;
&lt;div class=figure&gt;&lt;img src=/project/tdf_speed/tour_clean_tail.png alt=&#34;&#34; width=&#34;65%&#34; height=&#34;65%&#34;/&gt;&lt;/div&gt;
&lt;p&gt;So here is the data that we&amp;rsquo;re going to work with from now on. We&amp;rsquo;ve got winning time in hours, total distance in kilometers, winning margin in seconds, and then average speed in kilometers per hour. Let&amp;rsquo;s take a look now, and see how things have changed over time.&lt;/p&gt;
&lt;div class=figure&gt;&lt;img src=/project/tdf_speed/all_in_one.png&gt;&lt;/div&gt;
&lt;p&gt;There&amp;rsquo;s tons of interesting things going on here! One thing to note first, is the blue bars correspond to missing data - as in, years where there was no Tour de France, or the Tour was structured differently. First, between 1905 and 1912 the Tour was scored on points so there is no distance and time data there. Second (and third), the Tour was not run during either of the World Wars. So, brushing over that let&amp;rsquo;s dive into the data.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll focus first on the top and bottom graphs, because most of the interesting stuff, in my opinion, is in the middle. First, the Tour has been getting shorter since right around WW I. This isn&amp;rsquo;t entirely surprising because back in the early 20th century the Tour was envisioned as a savage race for only the hardest men, where only one man would actually make it to the end. So, over the years the Tour has gotten shorter but still a formidable distance.&lt;/p&gt;
&lt;p&gt;Second, the winning margin was huge back when the Tour was inhumanely difficult - the margin was in the realm of hours. However, since the 1950s the winning margin has been in the realm of minutes or seconds. We&amp;rsquo;ll dive a bit deeper into this later on.&lt;/p&gt;
&lt;p&gt;Lastly, the overall winning time and average speed. This is where stuff gets a bit more interesting! The overall winning time has been getting lower and lower, which makes sense given the tour has gotten shorter and faster. The tour has been getting consistently faster over the years, even over just the last few decades. In fact, the average speed was still around 32kmh in the early 1970s and was over 40 kmh the last few years. Thats nearly a 20% increase in average speed over four decades. Average speed seemed to increase pretty sharply from the 1960s to the early 2000s, but has seemed pretty consistent since then. I did, however, notice some interesting blips in the 1990s and 2000s. Let&amp;rsquo;s dive a little bit deeper in the speed data for those years.&lt;/p&gt;
&lt;div class=figure&gt;&lt;img src=/project/tdf_speed/zoomed.png&gt;&lt;/div&gt;
&lt;p&gt;When we zoom in on the last 46 years (1971-2017) we can see this pattern a little bit more clearly. When zoomed in, the pattern of the Tour getting faster looks a little bit less remarkable, but I think it&amp;rsquo;s still quite amazing when you unpack what these numbers are actually showing. I&amp;rsquo;m going to focus on the average speed column.&lt;/p&gt;
&lt;p&gt;While they don&amp;rsquo;t look like huge peaks, you&amp;rsquo;ll notice that the year of the Festina Affair and the last year of Lance (one year before Operacion Puerto) are the fastest edtions of the tour in the last half century - this is not over a trivial time frame. Even when you think about the advancements in bike, kit, and athlete training technology over the last decade, Lance in his last year was still faster than the current pros who have ten extra years of engineering underneath them.&lt;/p&gt;
&lt;p&gt;The other bit, is that even though the slope of the average speed line doesn&amp;rsquo;t look crazy - it&amp;rsquo;s actually quite steep. The average speed of the Tour has increased by about 6 km/h since the 1970s, which is an increase of about 17%. All this while the tour has remained relatively consistent in its distance of about 3,500 km.&lt;/p&gt;
&lt;p&gt;It would be nice to be able to factor in total elevation data (maybe they&amp;rsquo;re faster now because they&amp;rsquo;re climbing less), but I can&amp;rsquo;t find these data anywhere.&lt;/p&gt;
&lt;p&gt;The general pattern seems to me to be twofold: (1) The tour is getting faster and faster and (2) There were relatively big drops in average speed after each major doping scandal, followed by slow and steady increases in speed (including over the last twelve years).&lt;/p&gt;
&lt;p&gt;That being said, I don&amp;rsquo;t think the Tour will actually get substantially faster without drastic changes to the route or UCI rules. For instance, I can&amp;rsquo;t see the average speed hitting the mid-forties.&lt;/p&gt;
&lt;p&gt;Lastly, I&amp;rsquo;m going to zoom in a bit on the winning margins.&lt;/p&gt;
&lt;div class=figure&gt;&lt;img src=/project/tdf_speed/margin.png&gt;&lt;/div&gt;
&lt;p&gt;When we zoom in on the winning margins, we can see there is still a slight slope down. The time gap to the winner is getting smaller and smaller over time. Again, it doesn&amp;rsquo;t seem like much but the slope of this line goes down from ten minutes in 1971 (~ 600 seconds) to just 54 seconds in 2017. Again, I don&amp;rsquo;t think there&amp;rsquo;s really anywhere to go from here though. I suspect we will just continue to see the Tour being won on margins of less than a minute for the foreseeable future, unless there are major shakeups to the UCI&amp;rsquo;s rules.&lt;/p&gt;
&lt;h2 id=&#34;limitations&#34;&gt;Limitations&lt;/h2&gt;
&lt;p&gt;The biggest limitation of these basic visualizations, particularly the data about average speed, is that I would like to factor in the elevation gain for a given tour. The speed info is hard to interpret without it. For instance, a tour with 10 flat sprinters stages is likely to be faster, on average, than a tour with only 3. This doesn&amp;rsquo;t mean riders are getting faster overall, it just means the structure of the Tour was weighted towards faster stages. Unfortunately, I cannot find these data anywhere. I think we can assume though, that changes in the structure of the Tour don&amp;rsquo;t explain all changes in speed over the last few decades, where the race is very much a climbers race.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
